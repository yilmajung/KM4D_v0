{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/yilmajung/KM4D_v0/blob/main/ksp_pilot_llama70b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KSP Knowledge Extraction with Llama 3.1 70B\n",
        "\n",
        "**Same pipeline as `ksp_pilot_complete.ipynb`, but using Llama 3.1 70B Instruct (open-source) instead of Claude Sonnet.**\n",
        "\n",
        "**Pipeline Overview:**\n",
        "For each chapter/sub-chapter of a KSP report, this notebook performs:\n",
        "1. **Taxonomy Classification** \u2014 tags sectors & keywords from the development cooperation taxonomy\n",
        "2. **Knowledge Type** \u2014 classifies as one of 4 knowledge types\n",
        "3. **Korean Policy Extraction** \u2014 extracts policy_name, year_initiated, organization, challenge_addressed, policy_instruments, sector\n",
        "4. **Theory Linking** \u2014 matches related theories from development economics textbooks (via RAG)\n",
        "\n",
        "**LLM Backend:**\n",
        "- Model: `meta-llama/Meta-Llama-3.1-70B-Instruct`\n",
        "- Quantization: 4-bit (NF4) via bitsandbytes \u2014 requires Colab Pro A100 GPU (40-80GB VRAM)\n",
        "- Local inference (no API costs, but requires Colab Pro for A100)\n",
        "\n",
        "**Prerequisites:**\n",
        "- Colab Pro/Pro+ with A100 GPU runtime\n",
        "- HuggingFace account with Llama 3.1 license accepted\n",
        "- `HF_TOKEN` in Colab Secrets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup & Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install required packages\n",
        "!pip install -q pymupdf pdfplumber sentence-transformers chromadb pandas numpy scikit-learn matplotlib seaborn plotly networkx\n",
        "!pip install -q transformers accelerate bitsandbytes torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mount Google Drive for file persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create project directory in Google Drive\n",
        "import os\n",
        "project_dir = '/content/drive/MyDrive/KM4D_v0'\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/data/raw/ksp_reports', exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/data/raw/textbooks', exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/data/processed', exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/data/results', exist_ok=True)\n",
        "os.makedirs(f'{project_dir}/vector_db', exist_ok=True)\n",
        "\n",
        "print('Project directory created in Google Drive')\n",
        "print(f'  Location: {project_dir}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import json\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Central configuration for the chapter-level analysis pipeline (Llama 3.1 variant).\"\"\"\n",
        "\n",
        "    # Directories\n",
        "    project_dir: str = project_dir\n",
        "    ksp_dir: str = f'{project_dir}/data/raw/ksp_reports'\n",
        "    textbook_dir: str = f'{project_dir}/data/raw/textbooks'\n",
        "    processed_dir: str = f'{project_dir}/data/processed'\n",
        "    results_dir: str = f'{project_dir}/data/results'\n",
        "    vector_db_dir: str = f'{project_dir}/vector_db'\n",
        "\n",
        "    # Textbook chunking parameters\n",
        "    textbook_chunk_size: int = 768\n",
        "    chunk_overlap: int = 50\n",
        "\n",
        "    # Embedding model\n",
        "    embedding_model: str = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "    # LLM (Llama 3.1 70B Instruct, 4-bit quantized)\n",
        "    llm_model: str = 'meta-llama/Llama-3.1-70B-Instruct'\n",
        "    quantization: str = '4bit'\n",
        "    temperature: float = 0.1\n",
        "    max_new_tokens: int = 4096\n",
        "    max_content_chars: int = 15000  # same as Claude version (70B handles long context well)\n",
        "\n",
        "    # Retrieval parameters (textbook only)\n",
        "    textbook_top_k: int = 3\n",
        "\n",
        "    # Textbook collection\n",
        "    textbook_collection: str = 'textbooks_pilot'\n",
        "\n",
        "    # Output file suffix (to keep separate from Claude results)\n",
        "    output_suffix: str = '_llama70b'\n",
        "\n",
        "config = Config()\n",
        "print('Config initialized')\n",
        "print(f'  LLM: {config.llm_model} ({config.quantization})')\n",
        "print(f'  Max content chars: {config.max_content_chars}')\n",
        "print(f'  Max new tokens: {config.max_new_tokens}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# HuggingFace Token Setup\n",
        "# Llama 3.1 is a gated model - you must:\n",
        "# 1. Create a HuggingFace account at https://huggingface.co\n",
        "# 2. Accept the Llama 3.1 license at https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct\n",
        "# 3. Create an access token at https://huggingface.co/settings/tokens\n",
        "# 4. Add it as HF_TOKEN in Colab Secrets (key icon in sidebar)\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "    print('HuggingFace token loaded from Colab secrets')\n",
        "except:\n",
        "    print('No HF_TOKEN found in Colab secrets')\n",
        "    HF_TOKEN = input('Enter HuggingFace token: ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Taxonomy Reference & Knowledge Types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Full 6-sector, 4-level taxonomy from the Development Cooperation Taxonomy\n",
        "# Structure: Sector -> Sub-sector L1 -> Sub-sector L2 -> Keywords\n",
        "\n",
        "TAXONOMY = {\n",
        "    '(1) Economic Policy': {\n",
        "        'Macroeconomic Policy & Stability': {\n",
        "            'Fiscal Policy': [\n",
        "                'Public Finance Management (PFM)',\n",
        "                'Tax Policy & Administration',\n",
        "                'Public Expenditure Management',\n",
        "                'Sovereign Debt Management',\n",
        "                'Domestic Revenue Mobilization'\n",
        "            ],\n",
        "            'Monetary & Financial Policy': [\n",
        "                'Financial Sector Development',\n",
        "                'Monetary Policy & Central Banking',\n",
        "                'Financial Stability & Regulation',\n",
        "                'Financial Inclusion'\n",
        "            ]\n",
        "        },\n",
        "        'Inclusive & Sustainable Growth': {\n",
        "            'Growth Diagnostics & Strategy': [\n",
        "                'Economic Growth Strategy',\n",
        "                'Structural Reform',\n",
        "                'Productivity Analysis',\n",
        "                'Green Growth'\n",
        "            ],\n",
        "            'Poverty & Inequality': [\n",
        "                'Poverty Reduction Strategy',\n",
        "                'Inequality Analysis (Income, Wealth)',\n",
        "                'Social Protection Systems'\n",
        "            ]\n",
        "        },\n",
        "        'Investment & Private Sector Dev.': {\n",
        "            'Investment Climate': [\n",
        "                'Investment Policy & Promotion',\n",
        "                'Business Environment Reform',\n",
        "                'Competition Policy',\n",
        "                'Corporate Governance'\n",
        "            ],\n",
        "            'Private Sector Support': [\n",
        "                'Small & Medium Enterprise (SME) Dev.',\n",
        "                'Foreign Direct Investment (FDI)',\n",
        "                'Public-Private Partnerships (PPPs)',\n",
        "                'Entrepreneurship'\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    '(2) Social Services': {\n",
        "        'Education': {\n",
        "            'Education Policy & Systems': [\n",
        "                'Education Policy & Administration',\n",
        "                'Education Sector Planning',\n",
        "                'Teacher Training',\n",
        "                'Educational Facilities'\n",
        "            ],\n",
        "            'Levels of Education': [\n",
        "                'Early Childhood Education',\n",
        "                'Primary Education',\n",
        "                'Secondary Education',\n",
        "                'Higher Education',\n",
        "                'Vocational Training (TVET)',\n",
        "                'Adult Education & Lifelong Learning'\n",
        "            ]\n",
        "        },\n",
        "        'Health': {\n",
        "            'Health Policy & Systems': [\n",
        "                'Health Policy & Administration',\n",
        "                'Health Systems Strengthening',\n",
        "                'Health Financing & Insurance',\n",
        "                'Health Workforce Development',\n",
        "                'Digital Health'\n",
        "            ],\n",
        "            'Health Services & Outcomes': [\n",
        "                'Basic Health Care',\n",
        "                'Maternal & Child Health',\n",
        "                'Infectious Disease Control (HIV/AIDS, TB)',\n",
        "                'Non-Communicable Diseases (NCDs)',\n",
        "                'Mental Health',\n",
        "                'Pandemic Preparedness & Response'\n",
        "            ]\n",
        "        },\n",
        "        'Social Protection & Inclusion': {\n",
        "            'Social Welfare Services': [\n",
        "                'Social/Welfare Services',\n",
        "                'Child Protection',\n",
        "                'Services for the Elderly & Disabled'\n",
        "            ]\n",
        "        },\n",
        "        'Cross-Cutting Social Issues': {\n",
        "            'Gender Equality & Empowerment': [\n",
        "                'Gender Equality',\n",
        "                \"Women's Economic Empowerment\",\n",
        "                'Ending Violence Against Women & Girls',\n",
        "                'Youth Development & Employment',\n",
        "                'Food Security & Nutrition'\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    '(3) Digital Innovation': {\n",
        "        'Digital Policy & Governance': {\n",
        "            'Digital Strategy & Regulation': [\n",
        "                'National Digital Strategy',\n",
        "                'Data Governance & Privacy',\n",
        "                'Cybersecurity Policy',\n",
        "                'AI Governance & Ethics',\n",
        "                'Digital Taxation'\n",
        "            ]\n",
        "        },\n",
        "        'Digital Infrastructure': {\n",
        "            'Connectivity': [\n",
        "                'Universal Connectivity',\n",
        "                'Broadband Infrastructure',\n",
        "                'Mobile Networks (5G)'\n",
        "            ],\n",
        "            'Core Digital Systems': [\n",
        "                'Digital Public Infrastructure (DPI)',\n",
        "                'Digital Identity Systems',\n",
        "                'Digital Payment Systems'\n",
        "            ]\n",
        "        },\n",
        "        'Digital Transformation': {\n",
        "            'Digital Inclusion & Skills': [\n",
        "                'Digital Literacy & Skills',\n",
        "                'Addressing the Digital Divide',\n",
        "                'Gender and Digital Inclusion'\n",
        "            ],\n",
        "            'Digital Economy & Services': [\n",
        "                'E-Commerce',\n",
        "                'Digital Financial Services (FinTech)',\n",
        "                'E-Government Services',\n",
        "                'Digital Entrepreneurship',\n",
        "                'Platform Economy'\n",
        "            ]\n",
        "        },\n",
        "        'Emerging Technologies': {\n",
        "            'Technology Adoption': [\n",
        "                'Artificial Intelligence (AI)',\n",
        "                'Internet of Things (IoT)',\n",
        "                'Blockchain',\n",
        "                'Big Data Analytics'\n",
        "            ],\n",
        "            'Digital Public Goods': [\n",
        "                'Open Source Software & Data',\n",
        "                'Open Standards'\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    '(4) Production & Trade': {\n",
        "        'Agriculture, Forestry & Fisheries': {\n",
        "            'Agricultural Development': [\n",
        "                'Agricultural Policy & Administration',\n",
        "                'Agricultural Research & Extension',\n",
        "                'Sustainable Agriculture',\n",
        "                'Agricultural Value Chains',\n",
        "                'Smallholder Farmer Support'\n",
        "            ],\n",
        "            'Forestry & Fisheries': [\n",
        "                'Forestry Development',\n",
        "                'Sustainable Forest Management',\n",
        "                'Fisheries Development',\n",
        "                'Aquaculture'\n",
        "            ]\n",
        "        },\n",
        "        'Industry & Services': {\n",
        "            'Industrial Development': [\n",
        "                'Industrial Policy',\n",
        "                'Manufacturing',\n",
        "                'Agro-industry',\n",
        "                'Mineral Resources & Mining'\n",
        "            ],\n",
        "            'Services Sector Development': [\n",
        "                'Tourism Development',\n",
        "                'Business & Other Services'\n",
        "            ]\n",
        "        },\n",
        "        'Trade Policy & Facilitation': {\n",
        "            'Trade Policy & Regulation': [\n",
        "                'Trade Policy & Regulations',\n",
        "                'WTO Accession & Compliance',\n",
        "                'Regional Trade Agreements'\n",
        "            ],\n",
        "            'Trade Facilitation': [\n",
        "                'Customs Modernization',\n",
        "                'Border Management',\n",
        "                'Aid for Trade',\n",
        "                'Global Value Chains (GVCs)'\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    '(5) Infrastructure': {\n",
        "        'Infrastructure Policy & Finance': {\n",
        "            'Infrastructure Governance': [\n",
        "                'Infrastructure Policy & Planning',\n",
        "                'Infrastructure Governance',\n",
        "                'Public Investment Management'\n",
        "            ],\n",
        "            'Infrastructure Finance': [\n",
        "                'Infrastructure Financing',\n",
        "                'Private Investment in Infrastructure',\n",
        "                'Blended Finance'\n",
        "            ]\n",
        "        },\n",
        "        'Transport': {\n",
        "            'Transport Policy & Planning': [\n",
        "                'Transport Policy & Administration',\n",
        "                'Urban Transport',\n",
        "                'Rural Transport'\n",
        "            ],\n",
        "            'Transport Modalities': [\n",
        "                'Road Transport',\n",
        "                'Rail Transport',\n",
        "                'Water Transport (Ports, Inland)',\n",
        "                'Air Transport'\n",
        "            ]\n",
        "        },\n",
        "        'Water & Sanitation': {\n",
        "            'Water & Sanitation Policy': [\n",
        "                'Water & Sanitation Policy',\n",
        "                'Integrated Water Resource Management'\n",
        "            ],\n",
        "            'Water & Sanitation Services': [\n",
        "                'Water Supply Systems',\n",
        "                'Sanitation & Hygiene (WASH)',\n",
        "                'Wastewater Management'\n",
        "            ]\n",
        "        },\n",
        "        'Urban & Rural Development': {\n",
        "            'Urban Development': [\n",
        "                'Urban Development & Management',\n",
        "                'Affordable Housing'\n",
        "            ],\n",
        "            'Rural Development': [\n",
        "                'Rural Development'\n",
        "            ]\n",
        "        }\n",
        "    },\n",
        "    '(6) Energy & Environment': {\n",
        "        'Environmental Policy & Management': {\n",
        "            'Environmental Governance': [\n",
        "                'Environmental Policy & Admin. Management',\n",
        "                'Biosafety & Biodiversity',\n",
        "                'Water Resources Protection',\n",
        "                'Circular Economy'\n",
        "            ]\n",
        "        },\n",
        "        'Climate Change': {\n",
        "            'Climate Change Mitigation': [\n",
        "                'Renewable Energy Generation',\n",
        "                'Energy Efficiency',\n",
        "                'Greenhouse Gas (GHG) Reduction',\n",
        "                'Carbon Markets & Pricing'\n",
        "            ],\n",
        "            'Climate Change Adaptation': [\n",
        "                'Climate Adaptation Strategies',\n",
        "                'Disaster Risk Reduction (DRR)',\n",
        "                'Climate Resilient Infrastructure',\n",
        "                'Climate Finance'\n",
        "            ]\n",
        "        },\n",
        "        'Energy': {\n",
        "            'Energy Policy & Access': [\n",
        "                'Energy Policy & Planning',\n",
        "                'Universal Energy Access',\n",
        "                'Just Energy Transition (JET)',\n",
        "                'Energy Sector Reform & Regulation'\n",
        "            ],\n",
        "            'Energy Generation & Supply': [\n",
        "                'Renewable Energy (Solar, Wind, Hydro)',\n",
        "                'Non-Renewable Energy (Fossil Fuels)',\n",
        "                'Energy Transmission & Distribution'\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Knowledge types for classifying chapter content\n",
        "KNOWLEDGE_TYPES = [\n",
        "    'Policy Entrepreneurship (leadership)',\n",
        "    'Policy Implementation & Coordinating Mechanism',\n",
        "    'Organizational/Individual Capacity',\n",
        "    'Technical Know-how'\n",
        "]\n",
        "\n",
        "\n",
        "def taxonomy_to_prompt_string() -> str:\n",
        "    \"\"\"Convert TAXONOMY dict to a readable string for LLM prompts.\"\"\"\n",
        "    lines = []\n",
        "    for sector, l1_dict in TAXONOMY.items():\n",
        "        lines.append(f'\\n{sector}')\n",
        "        for l1, l2_dict in l1_dict.items():\n",
        "            lines.append(f'  {l1}')\n",
        "            for l2, keywords in l2_dict.items():\n",
        "                lines.append(f'    {l2}')\n",
        "                for kw in keywords:\n",
        "                    lines.append(f'      - {kw}')\n",
        "    return '\\n'.join(lines)\n",
        "\n",
        "\n",
        "def get_all_valid_keywords() -> set:\n",
        "    \"\"\"Get flat set of all valid keywords for validation.\"\"\"\n",
        "    keywords = set()\n",
        "    for sector, l1_dict in TAXONOMY.items():\n",
        "        for l1, l2_dict in l1_dict.items():\n",
        "            for l2, kw_list in l2_dict.items():\n",
        "                keywords.update(kw_list)\n",
        "    return keywords\n",
        "\n",
        "\n",
        "all_keywords = get_all_valid_keywords()\n",
        "print(f'Taxonomy loaded: {len(TAXONOMY)} sectors, {len(all_keywords)} keywords')\n",
        "print(f'Knowledge types: {len(KNOWLEDGE_TYPES)}')\n",
        "print()\n",
        "for sector in TAXONOMY:\n",
        "    n_kw = sum(len(kw) for l1 in TAXONOMY[sector].values() for kw in l1.values())\n",
        "    print(f'  {sector}: {n_kw} keywords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Chapter Extraction from KSP Reports (TOC Parsing)\n",
        "\n",
        "Extracts chapters by parsing the Table of Contents pages:\n",
        "- Detects dot-leader patterns (`.`, `\u2026`, `\u00b7`, `\u2219`) to find TOC pages\n",
        "- Filters out figure/table list pages\n",
        "- Handles multi-line titles and page number resolution\n",
        "- Truncates at page wrap-around to exclude appendix table/figure lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "\n",
        "class TOCChapterExtractor:\n",
        "    \"\"\"Extract chapters from KSP reports by parsing the Table of Contents pages.\n",
        "\n",
        "    Approach:\n",
        "    - Find TOC pages by detecting dot-leader patterns (., \u2026, \u00b7, \u2219)\n",
        "    - Filter out figure/table list pages\n",
        "    - Parse TOC entries with multi-line title support\n",
        "    - Resolve TOC page numbers to actual PDF page indices\n",
        "    - Truncate at page wrap-around (catches table/figure list residue)\n",
        "    - Extract text for each chapter by page range\n",
        "\n",
        "    Returns dicts compatible with the downstream ChapterAnalyzer pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    # 2+ repeated dot-leader chars (covers ., \u2026, \u00b7, \u2219 across all report styles)\n",
        "    DOT_LEADER_RE = re.compile(r'[.\u2026\u00b7\u2219\\u00B7\\u2026\\u2219]{2,}')\n",
        "    PAGE_NUM_RE = re.compile(r'[.\u2026\u00b7\u2219\\u00B7\\u2026\\u2219\\s]*(\\d{1,3})\\s*$')\n",
        "    LIST_HEADER_RE = re.compile(\n",
        "        r'List\\s+of\\s+(Tables?|Figures?|Pictures?|Exhibits?)', re.IGNORECASE\n",
        "    )\n",
        "\n",
        "    def __init__(self, pdf_path: str):\n",
        "        self.pdf_path = pdf_path\n",
        "        self.filename = Path(pdf_path).stem\n",
        "        self.doc = fitz.open(pdf_path)\n",
        "        self.metadata = self._extract_metadata()\n",
        "\n",
        "    def close(self):\n",
        "        self.doc.close()\n",
        "\n",
        "    def _extract_metadata(self) -> Dict:\n",
        "        \"\"\"Extract metadata from filename (YYYY_CCC_Title.pdf).\"\"\"\n",
        "        pattern = r'(\\d{4})_([A-Z]{3})_(.+)'\n",
        "        match = re.match(pattern, self.filename)\n",
        "        if match:\n",
        "            return {\n",
        "                'year': match.group(1),\n",
        "                'country': match.group(2),\n",
        "                'title': match.group(3).replace('_', ' '),\n",
        "                'filename': self.filename\n",
        "            }\n",
        "        return {'filename': self.filename}\n",
        "\n",
        "    def _find_toc_pages(self) -> List[int]:\n",
        "        \"\"\"Find content TOC pages, excluding figure/table list pages.\"\"\"\n",
        "        candidates = []\n",
        "        max_page = min(len(self.doc), max(20, int(len(self.doc) * 0.15)))\n",
        "\n",
        "        for pg_idx in range(max_page):\n",
        "            text = self.doc[pg_idx].get_text()\n",
        "            lines = text.split('\\n')\n",
        "            dot_count = sum(\n",
        "                1 for l in lines\n",
        "                if self.DOT_LEADER_RE.search(l) and self.PAGE_NUM_RE.search(l)\n",
        "            )\n",
        "            if dot_count >= 3:\n",
        "                is_list = bool(self.LIST_HEADER_RE.search(text[:500]))\n",
        "                if not is_list:\n",
        "                    fig_entries = sum(\n",
        "                        1 for l in lines\n",
        "                        if self.DOT_LEADER_RE.search(l) and self.PAGE_NUM_RE.search(l)\n",
        "                        and re.match(\n",
        "                            r'^\\s*(\\[?\\s*(Figure|Table|Picture|Exhibit)\\b|<\\s*(Table|Figure))',\n",
        "                            l.strip(), re.IGNORECASE\n",
        "                        )\n",
        "                    )\n",
        "                    if dot_count > 0 and fig_entries / dot_count > 0.5:\n",
        "                        is_list = True\n",
        "                candidates.append((pg_idx, is_list))\n",
        "\n",
        "        # Check adjacent pages for continuation\n",
        "        known = {pg for pg, _ in candidates}\n",
        "        for pg, _ in list(candidates):\n",
        "            for adj in [pg - 1, pg + 1]:\n",
        "                if 0 <= adj < max_page and adj not in known:\n",
        "                    text = self.doc[adj].get_text()\n",
        "                    lines = text.split('\\n')\n",
        "                    dc = sum(\n",
        "                        1 for l in lines\n",
        "                        if self.DOT_LEADER_RE.search(l) and self.PAGE_NUM_RE.search(l)\n",
        "                    )\n",
        "                    if dc >= 2:\n",
        "                        is_list = bool(self.LIST_HEADER_RE.search(text[:500]))\n",
        "                        if not is_list:\n",
        "                            fe = sum(\n",
        "                                1 for l in lines\n",
        "                                if self.DOT_LEADER_RE.search(l) and self.PAGE_NUM_RE.search(l)\n",
        "                                and re.match(\n",
        "                                    r'^\\s*(\\[?\\s*(Figure|Table|Picture)\\b|<\\s*(Table|Figure))',\n",
        "                                    l.strip(), re.IGNORECASE\n",
        "                                )\n",
        "                            )\n",
        "                            if dc > 0 and fe / dc > 0.5:\n",
        "                                is_list = True\n",
        "                        candidates.append((adj, is_list))\n",
        "                        known.add(adj)\n",
        "\n",
        "        candidates.sort()\n",
        "        # Return only content pages; stop at first list page\n",
        "        content_pages = []\n",
        "        for pg, is_list in candidates:\n",
        "            if is_list:\n",
        "                break\n",
        "            content_pages.append(pg)\n",
        "        return content_pages\n",
        "\n",
        "    def _parse_toc_entries(self, toc_pages: List[int]) -> List[Dict]:\n",
        "        \"\"\"Parse TOC entries from identified TOC pages, handling multi-line titles.\"\"\"\n",
        "        entries = []\n",
        "\n",
        "        for pg_idx in toc_pages:\n",
        "            text = self.doc[pg_idx].get_text()\n",
        "            lines = text.split('\\n')\n",
        "\n",
        "            i = 0\n",
        "            while i < len(lines):\n",
        "                stripped = lines[i].strip()\n",
        "\n",
        "                # Skip empty lines, headers\n",
        "                if not stripped or stripped.lower() in ('contents', 'table of contents'):\n",
        "                    i += 1\n",
        "                    continue\n",
        "                if re.match(r'^Chapter\\s+\\d+\\s*$', stripped, re.IGNORECASE):\n",
        "                    i += 1\n",
        "                    continue\n",
        "\n",
        "                # --- Line has dot leaders ---\n",
        "                if self.DOT_LEADER_RE.search(stripped):\n",
        "                    pm = self.PAGE_NUM_RE.search(stripped)\n",
        "                    if pm:\n",
        "                        title = self.DOT_LEADER_RE.split(stripped)[0].strip()\n",
        "                        if not title or len(title) < 3:\n",
        "                            i += 1\n",
        "                            continue\n",
        "                        # Skip table/figure entries\n",
        "                        if re.match(\n",
        "                            r'^(<\\s*(Table|Figure)|\\[?\\s*(Figure|Table|Picture))',\n",
        "                            title, re.IGNORECASE\n",
        "                        ):\n",
        "                            i += 1\n",
        "                            continue\n",
        "                        title = re.sub(r'\\s+', ' ', title).rstrip('.\u2026\u00b7\u2219 ')\n",
        "                        entries.append({\n",
        "                            'title': title,\n",
        "                            'page_num': int(pm.group(1)),\n",
        "                            'level': self._detect_level(title)\n",
        "                        })\n",
        "                        i += 1\n",
        "                        continue\n",
        "\n",
        "                    # Dots but no page number \u2014 look ahead\n",
        "                    title_so_far = self.DOT_LEADER_RE.split(stripped)[0].strip()\n",
        "                    found = False\n",
        "                    for j in range(i + 1, min(i + 3, len(lines))):\n",
        "                        ns = lines[j].strip()\n",
        "                        if not ns:\n",
        "                            continue\n",
        "                        pm = self.PAGE_NUM_RE.search(ns)\n",
        "                        if pm:\n",
        "                            tp = self.DOT_LEADER_RE.split(ns)[0].strip()\n",
        "                            combined = (title_so_far + ' ' + tp).strip() if tp else title_so_far\n",
        "                            combined = re.sub(r'\\s+', ' ', combined).rstrip('.\u2026\u00b7\u2219 ')\n",
        "                            if len(combined) >= 3:\n",
        "                                entries.append({\n",
        "                                    'title': combined,\n",
        "                                    'page_num': int(pm.group(1)),\n",
        "                                    'level': self._detect_level(combined)\n",
        "                                })\n",
        "                            i = j + 1\n",
        "                            found = True\n",
        "                            break\n",
        "                        break\n",
        "                    if not found:\n",
        "                        i += 1\n",
        "                    continue\n",
        "\n",
        "                # --- No dots: check for multi-line title ---\n",
        "                found = False\n",
        "                combined = stripped\n",
        "                for j in range(i + 1, min(i + 4, len(lines))):\n",
        "                    ns = lines[j].strip()\n",
        "                    if not ns:\n",
        "                        continue\n",
        "                    if self.DOT_LEADER_RE.search(ns):\n",
        "                        pm = self.PAGE_NUM_RE.search(ns)\n",
        "                        if pm:\n",
        "                            tp = self.DOT_LEADER_RE.split(ns)[0].strip()\n",
        "                            # Don't concatenate if dotted line is a standalone numbered entry\n",
        "                            if re.match(r'^\\d+[\\.\\)]\\s', tp) or re.match(r'^[A-Z]\\.\\s', tp):\n",
        "                                break\n",
        "                            if tp:\n",
        "                                combined += ' ' + tp\n",
        "                            combined = re.sub(r'\\s+', ' ', combined).rstrip('.\u2026\u00b7\u2219 ')\n",
        "                            if len(combined) >= 3:\n",
        "                                entries.append({\n",
        "                                    'title': combined,\n",
        "                                    'page_num': int(pm.group(1)),\n",
        "                                    'level': self._detect_level(combined)\n",
        "                                })\n",
        "                            i = j + 1\n",
        "                            found = True\n",
        "                            break\n",
        "                    # If next line starts a new numbered entry, stop\n",
        "                    if re.match(r'^\\d+[\\.\\)]\\s', ns) or re.match(r'^[A-Z]\\.\\s', ns):\n",
        "                        break\n",
        "                    combined += ' ' + ns\n",
        "\n",
        "                if not found:\n",
        "                    i += 1\n",
        "\n",
        "        return entries\n",
        "\n",
        "    def _detect_level(self, title: str) -> int:\n",
        "        \"\"\"Detect hierarchy level from title numbering pattern.\"\"\"\n",
        "        t = title.strip()\n",
        "        if re.match(r'^Chapter\\s+\\d', t, re.IGNORECASE):\n",
        "            return 0\n",
        "        if re.match(r'^\\d{1,2}\\.\\s', t):\n",
        "            return 1\n",
        "        if re.match(r'^[A-Z]\\.\\s', t):\n",
        "            return 1\n",
        "        if re.match(r'^\\d+\\.\\d+\\.?\\s', t):\n",
        "            return 2\n",
        "        if re.match(r'^\\d+\\)\\s', t):\n",
        "            return 2\n",
        "        if re.match(r'^\\d+\\.\\d+\\.\\d+', t):\n",
        "            return 3\n",
        "        return 1\n",
        "\n",
        "    def _resolve_page_numbers(self, entries: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Resolve TOC page numbers to PDF page indices; truncate at page wrap-around.\"\"\"\n",
        "        if not entries:\n",
        "            return entries\n",
        "\n",
        "        # Detect page offset by sampling entries\n",
        "        offsets = []\n",
        "        indices = (\n",
        "            list(range(min(5, len(entries))))\n",
        "            + list(range(len(entries) // 2, min(len(entries) // 2 + 3, len(entries))))\n",
        "        )\n",
        "        for idx in indices:\n",
        "            entry = entries[idx]\n",
        "            st = re.sub(r'^\\d+[\\.\\)]\\s*', '', entry['title'])\n",
        "            st = re.sub(r'^[A-Z]\\.\\s*', '', st)\n",
        "            st = re.sub(r'^\\d+\\.\\d+\\.?\\s*', '', st)\n",
        "            if len(st) < 5:\n",
        "                continue\n",
        "            search = st[:40].strip()\n",
        "            exp = entry['page_num']\n",
        "            for pg in range(max(0, exp - 10), min(len(self.doc), exp + 10)):\n",
        "                if search in self.doc[pg].get_text():\n",
        "                    offsets.append(pg - exp)\n",
        "                    break\n",
        "\n",
        "        best = Counter(offsets).most_common(1)[0][0] if offsets else 0\n",
        "\n",
        "        for e in entries:\n",
        "            e['pdf_page'] = max(0, min(e['page_num'] + best, len(self.doc) - 1))\n",
        "\n",
        "        # Truncate at page wrap-around\n",
        "        mx = entries[0]['pdf_page']\n",
        "        for i in range(1, len(entries)):\n",
        "            if entries[i]['pdf_page'] < mx - 5:\n",
        "                return entries[:i]\n",
        "            mx = max(mx, entries[i]['pdf_page'])\n",
        "\n",
        "        return entries\n",
        "\n",
        "    def extract_chapters(self, level_filter: Optional[int] = 1) -> List[Dict]:\n",
        "        \"\"\"Extract chapters from PDF using TOC parsing.\n",
        "\n",
        "        Args:\n",
        "            level_filter: Max level to include (1 = chapters + sections, 2 = include\n",
        "                         subsections). None = all levels.\n",
        "\n",
        "        Returns:\n",
        "            List of dicts with keys: chapter_title, chapter_level, content,\n",
        "            page_start, page_end, content_length.\n",
        "        \"\"\"\n",
        "        toc_pages = self._find_toc_pages()\n",
        "        if not toc_pages:\n",
        "            print(f'  WARNING: No TOC pages found in {self.filename}')\n",
        "            return []\n",
        "\n",
        "        print(f'  TOC pages (PDF): {[p + 1 for p in toc_pages]}')\n",
        "\n",
        "        entries = self._parse_toc_entries(toc_pages)\n",
        "        if not entries:\n",
        "            print(f'  WARNING: No TOC entries parsed')\n",
        "            return []\n",
        "\n",
        "        print(f'  Parsed {len(entries)} TOC entries')\n",
        "\n",
        "        if level_filter is not None:\n",
        "            entries = [e for e in entries if e['level'] <= level_filter]\n",
        "            print(f'  After level filter (\\u2264{level_filter}): {len(entries)}')\n",
        "\n",
        "        entries = self._resolve_page_numbers(entries)\n",
        "        print(f'  After page resolution: {len(entries)}')\n",
        "\n",
        "        # Build chapter dicts with text content\n",
        "        chapters = []\n",
        "        for i, entry in enumerate(entries):\n",
        "            ps = entry['pdf_page']\n",
        "            pe = entries[i + 1]['pdf_page'] if i + 1 < len(entries) else len(self.doc)\n",
        "\n",
        "            if pe <= ps:\n",
        "                continue  # Skip 0-content entries (parent headers)\n",
        "\n",
        "            content = '\\n'.join(\n",
        "                self.doc[pg].get_text()\n",
        "                for pg in range(ps, pe)\n",
        "                if 0 <= pg < len(self.doc)\n",
        "            )\n",
        "\n",
        "            chapters.append({\n",
        "                'chapter_title': entry['title'],\n",
        "                'chapter_level': entry['level'],\n",
        "                'content': content,\n",
        "                'page_start': ps + 1,  # 1-indexed for display\n",
        "                'page_end': pe,\n",
        "                'content_length': len(content)\n",
        "            })\n",
        "\n",
        "        return chapters\n",
        "\n",
        "    def get_summary(self) -> Dict:\n",
        "        \"\"\"Get a summary of the extracted chapters.\"\"\"\n",
        "        chapters = self.extract_chapters()\n",
        "        return {\n",
        "            'filename': self.filename,\n",
        "            'metadata': self.metadata,\n",
        "            'num_chapters': len(chapters),\n",
        "            'chapters': [\n",
        "                {\n",
        "                    'title': ch['chapter_title'],\n",
        "                    'level': ch['chapter_level'],\n",
        "                    'pages': f\"{ch['page_start']}-{ch['page_end']}\",\n",
        "                    'chars': ch['content_length']\n",
        "                }\n",
        "                for ch in chapters\n",
        "            ]\n",
        "        }\n",
        "\n",
        "\n",
        "print('TOCChapterExtractor class defined')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test TOC-based chapter extraction on one report\n",
        "ksp_pdfs = list(Path(config.ksp_dir).glob('*.pdf'))\n",
        "if ksp_pdfs:\n",
        "    test_pdf = str(ksp_pdfs[0])\n",
        "    print(f'Testing on: {Path(test_pdf).name}')\n",
        "    print()\n",
        "\n",
        "    extractor = TOCChapterExtractor(test_pdf)\n",
        "    print(f'Metadata: {extractor.metadata}')\n",
        "\n",
        "    chapters = extractor.extract_chapters(level_filter=1)\n",
        "    print(f'\\nExtracted {len(chapters)} chapters/sections')\n",
        "    print()\n",
        "    for i, ch in enumerate(chapters[:15]):\n",
        "        level_marker = '  ' * ch['chapter_level']\n",
        "        print(f\"{level_marker}[L{ch['chapter_level']}] {ch['chapter_title'][:80]}\")\n",
        "        print(f\"{level_marker}     Pages {ch['page_start']}-{ch['page_end']}, {ch['content_length']:,} chars\")\n",
        "    if len(chapters) > 15:\n",
        "        print(f'  ... and {len(chapters) - 15} more')\n",
        "\n",
        "    extractor.close()\n",
        "else:\n",
        "    print(f'No KSP reports found in: {config.ksp_dir}')\n",
        "    print('Please upload PDF reports to this directory')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Textbook Vector Store (for Theory Linking)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class VectorStore:\n",
        "    \"\"\"Manage embeddings and ChromaDB vector database for textbooks.\"\"\"\n",
        "\n",
        "    def __init__(self, collection_name: str, persist_directory: str = None):\n",
        "        self.collection_name = collection_name\n",
        "\n",
        "        # Initialize embedding model\n",
        "        print(f'Loading embedding model: {config.embedding_model}...')\n",
        "        self.embedding_model = SentenceTransformer(config.embedding_model)\n",
        "        print('Embedding model loaded')\n",
        "\n",
        "        # Initialize ChromaDB with persistence\n",
        "        if persist_directory is None:\n",
        "            persist_directory = config.vector_db_dir\n",
        "\n",
        "        self.client = chromadb.Client(Settings(\n",
        "            persist_directory=persist_directory,\n",
        "            anonymized_telemetry=False\n",
        "        ))\n",
        "\n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=collection_name)\n",
        "            print(f'Loaded existing collection: {collection_name}')\n",
        "        except:\n",
        "            self.collection = self.client.create_collection(\n",
        "                name=collection_name,\n",
        "                metadata={'description': f'Collection for {collection_name}'}\n",
        "            )\n",
        "            print(f'Created new collection: {collection_name}')\n",
        "\n",
        "    def add_documents(self, chunks: List[Dict], batch_size: int = 32):\n",
        "        \"\"\"Add document chunks to vector database.\"\"\"\n",
        "        documents = [chunk['text'] for chunk in chunks]\n",
        "        metadatas = [chunk['metadata'] for chunk in chunks]\n",
        "        ids = [chunk['metadata']['chunk_id'] for chunk in chunks]\n",
        "\n",
        "        print(f'Adding {len(documents)} documents to {self.collection_name}...')\n",
        "\n",
        "        all_embeddings = []\n",
        "        for i in tqdm(range(0, len(documents), batch_size), desc='Embedding'):\n",
        "            batch_docs = documents[i:i+batch_size]\n",
        "            embeddings = self.embedding_model.encode(\n",
        "                batch_docs, show_progress_bar=False, convert_to_numpy=True\n",
        "            ).tolist()\n",
        "            all_embeddings.extend(embeddings)\n",
        "\n",
        "        for i in tqdm(range(0, len(documents), batch_size), desc='Storing'):\n",
        "            batch_end = min(i + batch_size, len(documents))\n",
        "            self.collection.add(\n",
        "                documents=documents[i:batch_end],\n",
        "                embeddings=all_embeddings[i:batch_end],\n",
        "                metadatas=metadatas[i:batch_end],\n",
        "                ids=ids[i:batch_end]\n",
        "            )\n",
        "\n",
        "        print(f'Added {len(documents)} chunks to collection')\n",
        "\n",
        "    def search(self, query: str, n_results: int = 5, filter_dict: Dict = None) -> Dict:\n",
        "        \"\"\"Search for relevant chunks.\"\"\"\n",
        "        query_embedding = self.embedding_model.encode([query])[0].tolist()\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding],\n",
        "            n_results=n_results,\n",
        "            where=filter_dict\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Get collection statistics.\"\"\"\n",
        "        return {\n",
        "            'collection_name': self.collection_name,\n",
        "            'total_chunks': self.collection.count()\n",
        "        }\n",
        "\n",
        "\n",
        "print('VectorStore class defined')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize textbook vector store\n",
        "print('\\n' + '='*60)\n",
        "print('INITIALIZING TEXTBOOK VECTOR DATABASE')\n",
        "print('='*60)\n",
        "\n",
        "textbook_store = VectorStore(\n",
        "    collection_name=config.textbook_collection,\n",
        "    persist_directory=config.vector_db_dir\n",
        ")\n",
        "\n",
        "print(f'\\nTextbook store initialized')\n",
        "print(f'  ChromaDB persisted to: {config.vector_db_dir}')\n",
        "print(f'  Current stats: {textbook_store.get_stats()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q langchain langchain-text-splitters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Index textbooks (run once; skip if already indexed)\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "def process_and_index_textbooks():\n",
        "    \"\"\"Process textbooks and add to vector database.\"\"\"\n",
        "    textbook_dir = Path(config.textbook_dir)\n",
        "    pdf_files = list(textbook_dir.glob('*.pdf'))\n",
        "\n",
        "    print(f'\\nFound {len(pdf_files)} textbooks')\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config.textbook_chunk_size,\n",
        "        chunk_overlap=config.chunk_overlap,\n",
        "        separators=['\\n\\n', '\\n', '. ', ' ', ''],\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    for pdf_path in pdf_files:\n",
        "        print(f'\\nProcessing: {pdf_path.name}')\n",
        "\n",
        "        # Extract text with sections via PyMuPDF\n",
        "        doc = fitz.open(str(pdf_path))\n",
        "        sections = []\n",
        "        current_section = {'title': 'Introduction', 'content': '', 'page': 1}\n",
        "\n",
        "        for page_num, page in enumerate(doc, 1):\n",
        "            blocks = page.get_text('dict')['blocks']\n",
        "            for block in blocks:\n",
        "                if 'lines' not in block:\n",
        "                    continue\n",
        "                for line in block['lines']:\n",
        "                    text = ''\n",
        "                    font_size = 0\n",
        "                    for span in line['spans']:\n",
        "                        text += span['text']\n",
        "                        font_size = max(font_size, span['size'])\n",
        "                    text = text.strip()\n",
        "                    if not text:\n",
        "                        continue\n",
        "                    if font_size > 12 and len(text) < 100:\n",
        "                        if current_section['content'].strip():\n",
        "                            sections.append(current_section)\n",
        "                        current_section = {'title': text, 'content': '', 'page': page_num}\n",
        "                    else:\n",
        "                        current_section['content'] += text + '\\n'\n",
        "\n",
        "        if current_section['content'].strip():\n",
        "            sections.append(current_section)\n",
        "        doc.close()\n",
        "\n",
        "        print(f'  Extracted {len(sections)} sections')\n",
        "\n",
        "        # Chunk sections\n",
        "        filename = pdf_path.stem\n",
        "        metadata_base = {'source_type': 'textbook', 'filename': filename}\n",
        "\n",
        "        for s_idx, section in enumerate(sections):\n",
        "            sub_chunks = splitter.split_text(section['content'])\n",
        "            for c_idx, chunk_text in enumerate(sub_chunks):\n",
        "                all_chunks.append({\n",
        "                    'text': chunk_text,\n",
        "                    'metadata': {\n",
        "                        **metadata_base,\n",
        "                        'section_title': section['title'],\n",
        "                        'section_index': s_idx,\n",
        "                        'section_page': section['page'],\n",
        "                        'chunk_index': c_idx,\n",
        "                        'chunk_id': f'{filename}_s{s_idx}_c{c_idx}'\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        print(f'  Created {sum(1 for c in all_chunks if c[\"metadata\"][\"filename\"] == filename)} chunks')\n",
        "\n",
        "    # Add to vector database\n",
        "    if all_chunks:\n",
        "        textbook_store.add_documents(all_chunks)\n",
        "\n",
        "        output_path = f'{config.processed_dir}/textbook_chunks.json'\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(all_chunks, f, indent=2)\n",
        "        print(f'\\nSaved processed chunks to: {output_path}')\n",
        "\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "# Check if textbooks are already indexed\n",
        "stats = textbook_store.get_stats()\n",
        "if stats['total_chunks'] > 0:\n",
        "    print(f'\\nTextbooks already indexed: {stats[\"total_chunks\"]} chunks')\n",
        "    print('  Skipping re-indexing. Delete collection to re-index.')\n",
        "else:\n",
        "    textbook_pdfs = list(Path(config.textbook_dir).glob('*.pdf'))\n",
        "    if len(textbook_pdfs) == 0:\n",
        "        print(f'\\nNo textbooks found in: {config.textbook_dir}')\n",
        "        print('Please upload textbook PDFs to this directory')\n",
        "    else:\n",
        "        print(f'\\nFound {len(textbook_pdfs)} textbooks. Indexing...')\n",
        "        textbook_chunks = process_and_index_textbooks()\n",
        "        print(f'\\nTextbook Collection: {textbook_store.get_stats()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Process All KSP Reports (Chapter Extraction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_all_chapters() -> Dict[str, List[Dict]]:\n",
        "    \"\"\"Extract chapters from all KSP PDF reports using TOC parsing.\n",
        "\n",
        "    Returns dict mapping report_id -> {'metadata': ..., 'chapters': [...]}.\n",
        "    \"\"\"\n",
        "    ksp_dir = Path(config.ksp_dir)\n",
        "    pdf_files = sorted(ksp_dir.glob('*.pdf'))\n",
        "\n",
        "    print(f'Found {len(pdf_files)} KSP reports')\n",
        "\n",
        "    all_reports = {}\n",
        "\n",
        "    for pdf_path in pdf_files:\n",
        "        print(f'\\n{\"=\"*60}')\n",
        "        print(f'Processing: {pdf_path.name}')\n",
        "        print('='*60)\n",
        "\n",
        "        extractor = TOCChapterExtractor(str(pdf_path))\n",
        "        chapters = extractor.extract_chapters(level_filter=1)\n",
        "\n",
        "        report_id = extractor.filename\n",
        "        all_reports[report_id] = {\n",
        "            'metadata': extractor.metadata,\n",
        "            'chapters': chapters\n",
        "        }\n",
        "\n",
        "        print(f'  Extracted {len(chapters)} chapters/sections')\n",
        "        total_chars = sum(ch['content_length'] for ch in chapters)\n",
        "        print(f'  Total text: {total_chars:,} characters')\n",
        "\n",
        "        # Show chapter outline\n",
        "        for ch in chapters[:15]:\n",
        "            indent = '  ' * ch['chapter_level']\n",
        "            print(f\"  {indent}[L{ch['chapter_level']}] {ch['chapter_title'][:70]}  ({ch['content_length']:,} chars)\")\n",
        "        if len(chapters) > 15:\n",
        "            print(f'  ... and {len(chapters) - 15} more')\n",
        "\n",
        "        extractor.close()\n",
        "\n",
        "    # Save chapter summaries\n",
        "    summary = {}\n",
        "    for report_id, data in all_reports.items():\n",
        "        summary[report_id] = {\n",
        "            'metadata': data['metadata'],\n",
        "            'num_chapters': len(data['chapters']),\n",
        "            'chapters': [\n",
        "                {\n",
        "                    'title': ch['chapter_title'],\n",
        "                    'level': ch['chapter_level'],\n",
        "                    'pages': f\"{ch['page_start']}-{ch['page_end']}\",\n",
        "                    'chars': ch['content_length']\n",
        "                }\n",
        "                for ch in data['chapters']\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    summary_path = f'{config.processed_dir}/chapter_summaries.json'\n",
        "    with open(summary_path, 'w') as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f'\\nChapter summaries saved to: {summary_path}')\n",
        "\n",
        "    return all_reports\n",
        "\n",
        "\n",
        "# Extract chapters from all reports\n",
        "print('\\n' + '='*60)\n",
        "print('PHASE 1: CHAPTER EXTRACTION (TOC-based)')\n",
        "print('='*60)\n",
        "\n",
        "all_reports = extract_all_chapters()\n",
        "\n",
        "total_chapters = sum(len(data['chapters']) for data in all_reports.values())\n",
        "print(f'\\nTotal: {total_chapters} chapters/sections from {len(all_reports)} reports')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Load Llama 3.1 Model\n",
        "\n",
        "Load Llama 3.1 70B Instruct with 4-bit quantization (NF4).\n",
        "This requires Colab Pro/Pro+ A100 GPU (40-80GB VRAM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "\n",
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_mem = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
        "    print(f'GPU: {gpu_name} ({gpu_mem:.1f} GB)')\n",
        "else:\n",
        "    raise RuntimeError('No GPU available. Go to Runtime > Change runtime type > A100 GPU (requires Colab Pro)')\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(f'\\nLoading model: {config.llm_model}')\n",
        "print('  Quantization: 4-bit NF4 with double quantization')\n",
        "print('  This may take 5-10 minutes on first run (downloading ~35GB)...')\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    config.llm_model,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Load model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.llm_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "# Create text-generation pipeline\n",
        "llm_pipeline = pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Report memory usage\n",
        "mem_used = torch.cuda.memory_allocated() / 1e9\n",
        "mem_total = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
        "print(f'\\nModel loaded successfully')\n",
        "print(f'  GPU memory: {mem_used:.1f} / {mem_total:.1f} GB ({mem_used/mem_total*100:.0f}%)')\n",
        "print(f'  Free: {mem_total - mem_used:.1f} GB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: LLM Classification + Extraction (Combined Prompt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "import gc\n",
        "\n",
        "\n",
        "def extract_json_from_text(text: str) -> Optional[Dict]:\n",
        "    \"\"\"Extract JSON from LLM output, handling common formatting issues.\n",
        "\n",
        "    Llama models sometimes wrap JSON in markdown fences, adds preamble text,\n",
        "    or produces slightly malformed JSON. This function tries multiple\n",
        "    extraction strategies.\n",
        "    \"\"\"\n",
        "    # Strategy 1: Direct parse\n",
        "    text = text.strip()\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "\n",
        "    # Strategy 2: Strip markdown fences\n",
        "    cleaned = text\n",
        "    if '```json' in cleaned:\n",
        "        cleaned = cleaned.split('```json', 1)[1]\n",
        "    elif '```' in cleaned:\n",
        "        cleaned = cleaned.split('```', 1)[1]\n",
        "    if cleaned.endswith('```'):\n",
        "        cleaned = cleaned[:-3]\n",
        "    cleaned = cleaned.strip()\n",
        "    try:\n",
        "        return json.loads(cleaned)\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "\n",
        "    # Strategy 3: Find outermost { ... } block\n",
        "    first_brace = text.find('{')\n",
        "    last_brace = text.rfind('}')\n",
        "    if first_brace != -1 and last_brace > first_brace:\n",
        "        candidate = text[first_brace:last_brace + 1]\n",
        "        try:\n",
        "            return json.loads(candidate)\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    # Strategy 4: Fix common issues (trailing commas, single quotes)\n",
        "    if first_brace != -1 and last_brace > first_brace:\n",
        "        candidate = text[first_brace:last_brace + 1]\n",
        "        # Remove trailing commas before } or ]\n",
        "        candidate = re.sub(r',\\s*([}\\]])', r'\\1', candidate)\n",
        "        try:\n",
        "            return json.loads(candidate)\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "class ChapterAnalyzer:\n",
        "    \"\"\"Analyze KSP report chapters using Llama 3.1 70B with a combined prompt.\n",
        "\n",
        "    For each chapter, performs:\n",
        "    1. Taxonomy sector/keyword classification\n",
        "    2. Knowledge type classification\n",
        "    3. Korean policy extraction\n",
        "    4. Theory linking (using textbook RAG context)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm_pipeline, tokenizer, textbook_store: VectorStore):\n",
        "        self.llm_pipeline = llm_pipeline\n",
        "        self.tokenizer = tokenizer\n",
        "        self.textbook_store = textbook_store\n",
        "        self.taxonomy_string = taxonomy_to_prompt_string()\n",
        "\n",
        "    def _get_theory_context(self, chapter_title: str, chapter_content: str) -> str:\n",
        "        \"\"\"Query textbook store for relevant theory passages.\"\"\"\n",
        "        content_snippet = chapter_content[:200].replace('\\n', ' ')\n",
        "        query = f'{chapter_title} {content_snippet}'\n",
        "\n",
        "        try:\n",
        "            results = self.textbook_store.search(\n",
        "                query=query,\n",
        "                n_results=config.textbook_top_k\n",
        "            )\n",
        "            if results['documents'][0]:\n",
        "                return '\\n\\n---\\n\\n'.join(results['documents'][0])\n",
        "        except Exception as e:\n",
        "            print(f'  Warning: Theory retrieval failed: {e}')\n",
        "\n",
        "        return ''\n",
        "\n",
        "    def _build_prompt(self, chapter_title: str, chapter_content: str,\n",
        "                      theory_context: str) -> str:\n",
        "        \"\"\"Build the combined classification + extraction prompt.\"\"\"\n",
        "\n",
        "        # Truncate long chapters\n",
        "        max_content_chars = config.max_content_chars\n",
        "        if len(chapter_content) > max_content_chars:\n",
        "            chapter_content = chapter_content[:max_content_chars] + '\\n\\n[... content truncated ...]'\n",
        "\n",
        "        theory_section = theory_context if theory_context else 'No textbook passages retrieved.'\n",
        "\n",
        "        return f\"\"\"You are analyzing a chapter from a KSP (Knowledge Sharing Program) development cooperation report.\n",
        "\n",
        "CHAPTER TITLE: {chapter_title}\n",
        "\n",
        "CHAPTER CONTENT:\n",
        "{chapter_content}\n",
        "\n",
        "RELATED THEORETICAL PASSAGES (from development economics textbooks):\n",
        "{theory_section}\n",
        "\n",
        "DEVELOPMENT COOPERATION TAXONOMY:\n",
        "{self.taxonomy_string}\n",
        "\n",
        "KNOWLEDGE TYPES:\n",
        "1. Policy Entrepreneurship (leadership)\n",
        "2. Policy Implementation & Coordinating Mechanism\n",
        "3. Organizational/Individual Capacity\n",
        "4. Technical Know-how\n",
        "\n",
        "TASK: Perform ALL of the following analyses on this chapter.\n",
        "\n",
        "1. TAXONOMY CLASSIFICATION: Identify which sector(s) and keywords from the taxonomy above best describe this chapter's content. A chapter may map to multiple sectors. Select the most specific keywords that apply.\n",
        "\n",
        "2. KNOWLEDGE TYPE: Classify this chapter as one of the 4 knowledge types listed above.\n",
        "\n",
        "3. KOREAN POLICY EXTRACTION: Extract any Korean government policies, programs, or initiatives described in this chapter. For each policy provide: policy_name, year_initiated (null if not stated), organization (responsible ministry/agency, null if not stated), challenge_addressed, policy_instruments (list of specific mechanisms), sector. Each policy MUST include an evidence_quote (verbatim from the chapter). If this chapter does NOT contain any Korean policy experiences, return \"Not Applicable\" for this field.\n",
        "\n",
        "4. THEORY LINKING: Based on the textbook passages provided, identify any related theoretical concepts or frameworks. If no relevant theory link exists, return \"Not Applicable\".\n",
        "\n",
        "OUTPUT FORMAT: Return ONLY valid JSON (no markdown fences, no preamble, no explanation). Use this exact structure:\n",
        "\n",
        "{{\n",
        "  \"taxonomy_classification\": {{\n",
        "    \"sectors\": [\n",
        "      {{\n",
        "        \"sector\": \"(N) Sector Name\",\n",
        "        \"sub_sector_l1\": \"Sub-sector Level 1 name\",\n",
        "        \"sub_sector_l2\": \"Sub-sector Level 2 name\",\n",
        "        \"keywords\": [\"Keyword 1\", \"Keyword 2\"]\n",
        "      }}\n",
        "    ],\n",
        "    \"knowledge_type\": \"one of the 4 knowledge types\",\n",
        "    \"confidence\": \"high\" | \"medium\" | \"low\",\n",
        "    \"reasoning\": \"Brief explanation of classification\"\n",
        "  }},\n",
        "  \"korean_policies\": [\n",
        "    {{\n",
        "      \"policy_name\": \"string\",\n",
        "      \"year_initiated\": integer or null,\n",
        "      \"organization\": \"string\" or null,\n",
        "      \"challenge_addressed\": \"string\",\n",
        "      \"policy_instruments\": [\"string\"],\n",
        "      \"sector\": \"string\",\n",
        "      \"evidence_quote\": \"verbatim quote from chapter\"\n",
        "    }}\n",
        "  ],\n",
        "  \"related_theories\": [\n",
        "    {{\n",
        "      \"theory\": \"Theory name and source\",\n",
        "      \"relevance\": \"How this theory relates to the chapter content\"\n",
        "    }}\n",
        "  ]\n",
        "}}\n",
        "\n",
        "CRITICAL RULES:\n",
        "- Use ONLY keywords that exist in the taxonomy above\n",
        "- If no Korean policies are found, set \"korean_policies\" to \"Not Applicable\"\n",
        "- If no theory links are found, set \"related_theories\" to \"Not Applicable\"\n",
        "- taxonomy_classification and knowledge_type are ALWAYS required\n",
        "- evidence_quote must be verbatim from the chapter content\n",
        "- Return valid JSON only, starting with {{ and ending with }}\"\"\"\n",
        "\n",
        "    def analyze_chapter(self, chapter: Dict, report_id: str) -> Dict:\n",
        "        \"\"\"Analyze a single chapter with the combined prompt.\"\"\"\n",
        "\n",
        "        chapter_title = chapter['chapter_title']\n",
        "        chapter_content = chapter['content']\n",
        "\n",
        "        # Get theory context from textbook store\n",
        "        theory_context = self._get_theory_context(chapter_title, chapter_content)\n",
        "\n",
        "        # Build prompt and format as chat messages\n",
        "        prompt = self._build_prompt(chapter_title, chapter_content, theory_context)\n",
        "        messages = [\n",
        "            {'role': 'system', 'content': 'You are a development economics research assistant. Always respond with valid JSON only.'},\n",
        "            {'role': 'user', 'content': prompt}\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            outputs = self.llm_pipeline(\n",
        "                messages,\n",
        "                max_new_tokens=config.max_new_tokens,\n",
        "                temperature=config.temperature,\n",
        "                do_sample=True,\n",
        "                return_full_text=False\n",
        "            )\n",
        "\n",
        "            content = outputs[0]['generated_text'].strip()\n",
        "\n",
        "            # Extract JSON from response\n",
        "            result = extract_json_from_text(content)\n",
        "\n",
        "            if result is None:\n",
        "                print(f'  JSON parse error for \"{chapter_title}\"')\n",
        "                print(f'  Response preview: {content[:200]}...')\n",
        "                return {\n",
        "                    'report_id': report_id,\n",
        "                    'chapter_title': chapter_title,\n",
        "                    'chapter_level': chapter['chapter_level'],\n",
        "                    'page_start': chapter['page_start'],\n",
        "                    'page_end': chapter['page_end'],\n",
        "                    'content_length': chapter['content_length'],\n",
        "                    'error': 'JSON parse error',\n",
        "                    'raw_response': content[:1000]\n",
        "                }\n",
        "\n",
        "            # Attach chapter metadata\n",
        "            return {\n",
        "                'report_id': report_id,\n",
        "                'chapter_title': chapter_title,\n",
        "                'chapter_level': chapter['chapter_level'],\n",
        "                'page_start': chapter['page_start'],\n",
        "                'page_end': chapter['page_end'],\n",
        "                'content_length': chapter['content_length'],\n",
        "                **result\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'  Inference error for \"{chapter_title}\": {e}')\n",
        "            return {\n",
        "                'report_id': report_id,\n",
        "                'chapter_title': chapter_title,\n",
        "                'chapter_level': chapter['chapter_level'],\n",
        "                'page_start': chapter['page_start'],\n",
        "                'page_end': chapter['page_end'],\n",
        "                'content_length': chapter['content_length'],\n",
        "                'error': str(e)\n",
        "            }\n",
        "        finally:\n",
        "            # Free GPU cache between chapters\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "# Initialize analyzer\n",
        "analyzer = ChapterAnalyzer(\n",
        "    llm_pipeline=llm_pipeline,\n",
        "    tokenizer=tokenizer,\n",
        "    textbook_store=textbook_store\n",
        ")\n",
        "print('ChapterAnalyzer initialized (Llama 3.1 70B backend)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Run Analysis on All Reports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_full_analysis(all_reports: Dict, analyzer: ChapterAnalyzer) -> List[Dict]:\n",
        "    \"\"\"Run combined classification + extraction on all chapters of all reports.\"\"\"\n",
        "\n",
        "    all_results = []\n",
        "    total_chapters = sum(len(data['chapters']) for data in all_reports.values())\n",
        "    processed = 0\n",
        "\n",
        "    for report_id, data in all_reports.items():\n",
        "        chapters = data['chapters']\n",
        "        print(f'\\n{\"=\"*60}')\n",
        "        print(f'ANALYZING: {report_id}')\n",
        "        print(f'  {len(chapters)} chapters to process')\n",
        "        print('='*60)\n",
        "\n",
        "        for i, chapter in enumerate(chapters):\n",
        "            processed += 1\n",
        "            print(f'\\n  [{processed}/{total_chapters}] \"{chapter[\"chapter_title\"][:60]}...\"')\n",
        "            print(f'    Pages {chapter[\"page_start\"]}-{chapter[\"page_end\"]}, {chapter[\"content_length\"]} chars')\n",
        "\n",
        "            start_time = time.time()\n",
        "            result = analyzer.analyze_chapter(chapter, report_id)\n",
        "            elapsed = time.time() - start_time\n",
        "            all_results.append(result)\n",
        "\n",
        "            # Show brief result\n",
        "            if 'taxonomy_classification' in result:\n",
        "                tc = result['taxonomy_classification']\n",
        "                sectors = [s['sector'] for s in tc.get('sectors', [])]\n",
        "                kt = tc.get('knowledge_type', 'N/A')\n",
        "                print(f'    Sectors: {sectors}')\n",
        "                print(f'    Knowledge type: {kt}')\n",
        "\n",
        "                policies = result.get('korean_policies', 'Not Applicable')\n",
        "                if isinstance(policies, list):\n",
        "                    print(f'    Korean policies: {len(policies)} found')\n",
        "                else:\n",
        "                    print(f'    Korean policies: {policies}')\n",
        "\n",
        "                print(f'    Inference time: {elapsed:.1f}s')\n",
        "            elif 'error' in result:\n",
        "                print(f'    ERROR: {result[\"error\"][:80]}')\n",
        "\n",
        "    # Save results (with _llama suffix to keep separate from Claude results)\n",
        "    output_path = f'{config.results_dir}/chapter_analysis{config.output_suffix}.json'\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print('ANALYSIS COMPLETE')\n",
        "    print('='*60)\n",
        "    print(f'Total chapters analyzed: {len(all_results)}')\n",
        "    print(f'Results saved to: {output_path}')\n",
        "\n",
        "    # Summary stats\n",
        "    errors = sum(1 for r in all_results if 'error' in r)\n",
        "    with_policies = sum(\n",
        "        1 for r in all_results\n",
        "        if isinstance(r.get('korean_policies'), list) and len(r['korean_policies']) > 0\n",
        "    )\n",
        "    total_policies = sum(\n",
        "        len(r['korean_policies'])\n",
        "        for r in all_results\n",
        "        if isinstance(r.get('korean_policies'), list)\n",
        "    )\n",
        "\n",
        "    print(f'\\nErrors: {errors}')\n",
        "    print(f'Chapters with Korean policies: {with_policies}')\n",
        "    print(f'Total Korean policies extracted: {total_policies}')\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# Run the analysis\n",
        "print('\\n' + '='*60)\n",
        "print('PHASE 2: CHAPTER-LEVEL CLASSIFICATION & EXTRACTION (Llama 3.1 70B)')\n",
        "print('='*60)\n",
        "\n",
        "analysis_results = run_full_analysis(all_reports, analyzer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Visualization & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def visualize_results(results: List[Dict]):\n",
        "    \"\"\"Generate visualizations from chapter analysis results.\"\"\"\n",
        "\n",
        "    # Filter out errors\n",
        "    valid = [r for r in results if 'taxonomy_classification' in r]\n",
        "    print(f'Visualizing {len(valid)} successfully analyzed chapters ({len(results) - len(valid)} errors)\\n')\n",
        "\n",
        "    if not valid:\n",
        "        print('No valid results to visualize')\n",
        "        return\n",
        "\n",
        "    suffix = config.output_suffix\n",
        "\n",
        "    # --- 1. Sector Distribution ---\n",
        "    sector_counts = Counter()\n",
        "    for r in valid:\n",
        "        for s in r['taxonomy_classification'].get('sectors', []):\n",
        "            sector_counts[s['sector']] += 1\n",
        "\n",
        "    if sector_counts:\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        sectors = sorted(sector_counts.keys())\n",
        "        counts = [sector_counts[s] for s in sectors]\n",
        "        labels = [s.replace('&', '&\\n') if len(s) > 25 else s for s in sectors]\n",
        "        ax.barh(labels, counts, color='steelblue')\n",
        "        ax.set_xlabel('Number of Chapters')\n",
        "        ax.set_title('Sector Distribution Across All Chapters (Llama 3.1 70B)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{config.results_dir}/sector_distribution{suffix}.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f'Saved sector_distribution{suffix}.png')\n",
        "\n",
        "    # --- 2. Knowledge Type Distribution ---\n",
        "    kt_counts = Counter()\n",
        "    for r in valid:\n",
        "        kt = r['taxonomy_classification'].get('knowledge_type', 'Unknown')\n",
        "        kt_counts[kt] += 1\n",
        "\n",
        "    if kt_counts:\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        kt_labels = list(kt_counts.keys())\n",
        "        kt_vals = [kt_counts[k] for k in kt_labels]\n",
        "        wrapped_labels = []\n",
        "        for label in kt_labels:\n",
        "            if len(label) > 35:\n",
        "                words = label.split()\n",
        "                mid = len(words) // 2\n",
        "                wrapped_labels.append(' '.join(words[:mid]) + '\\n' + ' '.join(words[mid:]))\n",
        "            else:\n",
        "                wrapped_labels.append(label)\n",
        "        ax.barh(wrapped_labels, kt_vals, color='coral')\n",
        "        ax.set_xlabel('Number of Chapters')\n",
        "        ax.set_title('Knowledge Type Distribution (Llama 3.1 70B)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'{config.results_dir}/knowledge_type_distribution{suffix}.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f'Saved knowledge_type_distribution{suffix}.png')\n",
        "\n",
        "    # --- 3. Sector x Knowledge Type Heatmap ---\n",
        "    if sector_counts and kt_counts:\n",
        "        cross_data = defaultdict(lambda: defaultdict(int))\n",
        "        for r in valid:\n",
        "            kt = r['taxonomy_classification'].get('knowledge_type', 'Unknown')\n",
        "            for s in r['taxonomy_classification'].get('sectors', []):\n",
        "                cross_data[s['sector']][kt] += 1\n",
        "\n",
        "        df_cross = pd.DataFrame(cross_data).fillna(0).astype(int)\n",
        "        if not df_cross.empty:\n",
        "            fig, ax = plt.subplots(figsize=(12, 6))\n",
        "            sns.heatmap(df_cross, annot=True, fmt='d', cmap='YlOrRd', ax=ax)\n",
        "            ax.set_title('Sector x Knowledge Type Heatmap (Llama 3.1 70B)')\n",
        "            ax.set_ylabel('Knowledge Type')\n",
        "            ax.set_xlabel('Sector')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'{config.results_dir}/sector_knowledge_heatmap{suffix}.png', dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f'Saved sector_knowledge_heatmap{suffix}.png')\n",
        "\n",
        "    # --- 4. Per-Report Summary ---\n",
        "    print('\\n' + '='*60)\n",
        "    print('PER-REPORT SUMMARY')\n",
        "    print('='*60)\n",
        "\n",
        "    report_groups = defaultdict(list)\n",
        "    for r in valid:\n",
        "        report_groups[r['report_id']].append(r)\n",
        "\n",
        "    for report_id, chapters in report_groups.items():\n",
        "        print(f'\\n--- {report_id} ---')\n",
        "        print(f'  Chapters analyzed: {len(chapters)}')\n",
        "\n",
        "        # Policies\n",
        "        total_policies = 0\n",
        "        for ch in chapters:\n",
        "            policies = ch.get('korean_policies', 'Not Applicable')\n",
        "            if isinstance(policies, list):\n",
        "                total_policies += len(policies)\n",
        "        print(f'  Korean policies extracted: {total_policies}')\n",
        "\n",
        "        # Top sectors\n",
        "        ch_sectors = Counter()\n",
        "        for ch in chapters:\n",
        "            for s in ch['taxonomy_classification'].get('sectors', []):\n",
        "                ch_sectors[s['sector']] += 1\n",
        "        if ch_sectors:\n",
        "            top = ch_sectors.most_common(3)\n",
        "            print(f'  Top sectors: {[\", \".join(f\"{s}({c})\" for s, c in top)]}')\n",
        "\n",
        "        # Knowledge types\n",
        "        ch_kt = Counter(ch['taxonomy_classification'].get('knowledge_type', 'Unknown') for ch in chapters)\n",
        "        print(f'  Knowledge types: {dict(ch_kt)}')\n",
        "\n",
        "        # Show policies\n",
        "        if total_policies > 0:\n",
        "            print(f'  Policies:')\n",
        "            for ch in chapters:\n",
        "                policies = ch.get('korean_policies', 'Not Applicable')\n",
        "                if isinstance(policies, list):\n",
        "                    for p in policies:\n",
        "                        yr = p.get('year_initiated', '?')\n",
        "                        print(f'    - {p[\"policy_name\"]} ({yr})')\n",
        "\n",
        "\n",
        "# Run visualization\n",
        "if 'analysis_results' in locals() and analysis_results:\n",
        "    visualize_results(analysis_results)\n",
        "else:\n",
        "    # Try loading from file\n",
        "    results_path = f'{config.results_dir}/chapter_analysis{config.output_suffix}.json'\n",
        "    if os.path.exists(results_path):\n",
        "        with open(results_path) as f:\n",
        "            analysis_results = json.load(f)\n",
        "        print(f'Loaded {len(analysis_results)} results from file')\n",
        "        visualize_results(analysis_results)\n",
        "    else:\n",
        "        print('No analysis results found. Run Section 9 first.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Detailed chapter-level view for one report\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "def show_detailed_report(results: List[Dict], report_id: str = None):\n",
        "    \"\"\"Show detailed chapter-by-chapter view for a specific report.\"\"\"\n",
        "\n",
        "    valid = [r for r in results if 'taxonomy_classification' in r]\n",
        "    if report_id:\n",
        "        report_chapters = [r for r in valid if r['report_id'] == report_id]\n",
        "    else:\n",
        "        if not valid:\n",
        "            print('No valid results')\n",
        "            return\n",
        "        report_id = valid[0]['report_id']\n",
        "        report_chapters = [r for r in valid if r['report_id'] == report_id]\n",
        "\n",
        "    print(f'\\nDETAILED VIEW: {report_id}')\n",
        "    print('='*80)\n",
        "\n",
        "    for ch in report_chapters:\n",
        "        indent = '  ' * ch.get('chapter_level', 0)\n",
        "        print(f'\\n{indent}[L{ch.get(\"chapter_level\", 0)}] {ch[\"chapter_title\"]}')\n",
        "        print(f'{indent}  Pages {ch[\"page_start\"]}-{ch[\"page_end\"]} | {ch[\"content_length\"]} chars')\n",
        "\n",
        "        tc = ch['taxonomy_classification']\n",
        "        for s in tc.get('sectors', []):\n",
        "            kws = ', '.join(s.get('keywords', []))\n",
        "            print(f'{indent}  Sector: {s[\"sector\"]} > {s.get(\"sub_sector_l1\", \"\")} > {s.get(\"sub_sector_l2\", \"\")}')\n",
        "            print(f'{indent}  Keywords: {kws}')\n",
        "        print(f'{indent}  Knowledge type: {tc.get(\"knowledge_type\", \"N/A\")}')\n",
        "        print(f'{indent}  Confidence: {tc.get(\"confidence\", \"N/A\")}')\n",
        "\n",
        "        policies = ch.get('korean_policies', 'Not Applicable')\n",
        "        if isinstance(policies, list) and policies:\n",
        "            for p in policies:\n",
        "                print(f'{indent}  Policy: {p[\"policy_name\"]} ({p.get(\"year_initiated\", \"?\")}) [{p.get(\"sector\", \"\")}]')\n",
        "                print(f'{indent}    Org: {p.get(\"organization\", \"N/A\")}')\n",
        "                print(f'{indent}    Challenge: {p.get(\"challenge_addressed\", \"N/A\")}')\n",
        "                instruments = p.get('policy_instruments', [])\n",
        "                if instruments:\n",
        "                    print(f'{indent}    Instruments: {\", \".join(instruments)}')\n",
        "                quote = p.get('evidence_quote', '')\n",
        "                if quote:\n",
        "                    print(f'{indent}    Evidence: \"{quote[:100]}...\"')\n",
        "        elif policies == 'Not Applicable':\n",
        "            print(f'{indent}  Korean policies: Not Applicable')\n",
        "\n",
        "        theories = ch.get('related_theories', 'Not Applicable')\n",
        "        if isinstance(theories, list) and theories:\n",
        "            for t in theories:\n",
        "                print(f'{indent}  Theory: {t.get(\"theory\", \"\")} - {t.get(\"relevance\", \"\")[:80]}')\n",
        "        elif theories == 'Not Applicable':\n",
        "            print(f'{indent}  Related theories: Not Applicable')\n",
        "\n",
        "    # Theory-Practice Network for this report\n",
        "    suffix = config.output_suffix\n",
        "    policies_for_network = []\n",
        "    for ch in report_chapters:\n",
        "        policies = ch.get('korean_policies', 'Not Applicable')\n",
        "        theories = ch.get('related_theories', 'Not Applicable')\n",
        "        if isinstance(policies, list):\n",
        "            for p in policies:\n",
        "                p_theories = theories if isinstance(theories, list) else []\n",
        "                policies_for_network.append((p, p_theories))\n",
        "\n",
        "    if policies_for_network:\n",
        "        G = nx.Graph()\n",
        "        for policy, theories in policies_for_network:\n",
        "            pname = policy['policy_name'][:35]\n",
        "            G.add_node(pname, node_type='policy')\n",
        "            for t in theories:\n",
        "                tname = t.get('theory', '')[:35]\n",
        "                if tname:\n",
        "                    G.add_node(tname, node_type='theory')\n",
        "                    G.add_edge(pname, tname)\n",
        "\n",
        "        if G.number_of_nodes() > 0:\n",
        "            plt.figure(figsize=(14, 10))\n",
        "            pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n",
        "            colors = ['lightblue' if G.nodes[n].get('node_type') == 'policy' else 'lightcoral' for n in G.nodes()]\n",
        "            nx.draw(G, pos, node_color=colors, with_labels=True, font_size=7,\n",
        "                    node_size=500, alpha=0.7, edge_color='gray')\n",
        "            plt.title(f'Theory-Practice Network: {report_id}\\n(Blue=Policies, Red=Theories) [Llama 3.1 70B]')\n",
        "            plt.tight_layout()\n",
        "            safe_name = report_id.replace(' ', '_')[:50]\n",
        "            plt.savefig(f'{config.results_dir}/network_{safe_name}{suffix}.png', dpi=150, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "# Show detailed view for each report\n",
        "if 'analysis_results' in locals() and analysis_results:\n",
        "    report_ids = list(set(r['report_id'] for r in analysis_results if 'taxonomy_classification' in r))\n",
        "    for rid in report_ids:\n",
        "        show_detailed_report(analysis_results, rid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 11: Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print('\\n' + '='*60)\n",
        "print('CHAPTER-LEVEL ANALYSIS COMPLETE (Llama 3.1 70B)')\n",
        "print('='*60)\n",
        "\n",
        "suffix = config.output_suffix\n",
        "print(f'\\nResults saved in: {config.project_dir}')\n",
        "print('\\nKey outputs:')\n",
        "print(f'  1. Chapter summaries: {config.processed_dir}/chapter_summaries.json')\n",
        "print(f'  2. Full analysis: {config.results_dir}/chapter_analysis{suffix}.json')\n",
        "print(f'  3. Visualizations: {config.results_dir}/*{suffix}.png')\n",
        "print(f'  4. Textbook vectors: {config.vector_db_dir}/')\n",
        "\n",
        "# Load and display summary stats\n",
        "if 'analysis_results' in locals() and analysis_results:\n",
        "    valid = [r for r in analysis_results if 'taxonomy_classification' in r]\n",
        "    errors = len(analysis_results) - len(valid)\n",
        "\n",
        "    total_policies = sum(\n",
        "        len(r['korean_policies'])\n",
        "        for r in valid\n",
        "        if isinstance(r.get('korean_policies'), list)\n",
        "    )\n",
        "\n",
        "    chapters_with_policies = sum(\n",
        "        1 for r in valid\n",
        "        if isinstance(r.get('korean_policies'), list) and len(r['korean_policies']) > 0\n",
        "    )\n",
        "\n",
        "    chapters_with_theories = sum(\n",
        "        1 for r in valid\n",
        "        if isinstance(r.get('related_theories'), list) and len(r['related_theories']) > 0\n",
        "    )\n",
        "\n",
        "    print(f'\\nSummary Statistics:')\n",
        "    print(f'  Model: {config.llm_model} ({config.quantization})')\n",
        "    print(f'  Reports processed: {len(set(r[\"report_id\"] for r in analysis_results))}')\n",
        "    print(f'  Chapters analyzed: {len(valid)} ({errors} errors)')\n",
        "    print(f'  JSON parse success rate: {len(valid)}/{len(analysis_results)} ({len(valid)/len(analysis_results)*100:.0f}%)')\n",
        "    print(f'  Korean policies extracted: {total_policies} (in {chapters_with_policies} chapters)')\n",
        "    print(f'  Chapters with theory links: {chapters_with_theories}')\n",
        "\n",
        "    # Sector coverage\n",
        "    all_sectors = Counter()\n",
        "    for r in valid:\n",
        "        for s in r['taxonomy_classification'].get('sectors', []):\n",
        "            all_sectors[s['sector']] += 1\n",
        "    print(f'  Sector coverage: {len(all_sectors)} sectors used')\n",
        "    for sector, count in all_sectors.most_common():\n",
        "        print(f'    {sector}: {count} chapters')\n",
        "\n",
        "print('\\nNext steps:')\n",
        "print('  1. Compare chapter_analysis_llama70b.json with chapter_analysis.json (Claude)')\n",
        "print('  2. Evaluate taxonomy classification accuracy vs Claude')\n",
        "print('  3. Assess Korean policy extraction quality')\n",
        "print('  4. Compare with 8B results to assess quality gain from larger model')\n",
        "\n",
        "print('\\n' + '='*60)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}