{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KSP Knowledge Extraction - Pilot Study\n",
    "\n",
    "Complete Implementation for Google Colab\n",
    "\n",
    "**PILOT STUDY SETUP:**\n",
    "- 4 KSP advisory reports\n",
    "- 2 development economics textbooks\n",
    "- Dual-collection RAG with ChromaDB\n",
    "- LLM extraction using Anthropic Claude API\n",
    "- Full evaluation and visualization\n",
    "\n",
    "**GOOGLE COLAB USAGE:**\n",
    "- Runtime -> Change runtime type -> T4 GPU (for faster embeddings)\n",
    "- Files will be saved to Google Drive for persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pymupdf pdfplumber sentence-transformers chromadb anthropic pandas numpy scikit-learn matplotlib seaborn plotly networkx umap-learn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for file persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project directory in Google Drive\n",
    "import os\n",
    "project_dir = '/content/drive/MyDrive/KSP_Pilot'\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/raw/ksp_reports', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/raw/textbooks', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/processed', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/results', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/gold_standard', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/vector_db', exist_ok=True)\n",
    "\n",
    "print(\"\\u2713 Project directory created in Google Drive\")\n",
    "print(f\"  Location: {project_dir}\")\n",
    "print(\"\\nNext step: Upload your PDFs to:\")\n",
    "print(f\"  - KSP reports: {project_dir}/data/raw/ksp_reports/\")\n",
    "print(f\"  - Textbooks: {project_dir}/data/raw/textbooks/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the pilot study.\"\"\"\n",
    "    \n",
    "    # Directories\n",
    "    project_dir: str = project_dir\n",
    "    ksp_dir: str = f\"{project_dir}/data/raw/ksp_reports\"\n",
    "    textbook_dir: str = f\"{project_dir}/data/raw/textbooks\"\n",
    "    processed_dir: str = f\"{project_dir}/data/processed\"\n",
    "    results_dir: str = f\"{project_dir}/data/results\"\n",
    "    vector_db_dir: str = f\"{project_dir}/vector_db\"\n",
    "    \n",
    "    # Chunking parameters\n",
    "    ksp_chunk_size: int = 512\n",
    "    textbook_chunk_size: int = 768\n",
    "    chunk_overlap: int = 50\n",
    "    \n",
    "    # Embedding model\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    # LLM API\n",
    "    llm_provider: str = \"anthropic\"  # or \"openai\"\n",
    "    llm_model: str = \"claude-sonnet-4-20250514\"\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 4000\n",
    "    \n",
    "    # Retrieval parameters\n",
    "    ksp_top_k: int = 5\n",
    "    textbook_top_k: int = 3\n",
    "    \n",
    "    # Collections\n",
    "    ksp_collection: str = \"ksp_reports_pilot\"\n",
    "    textbook_collection: str = \"textbooks_pilot\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key Setup\n",
    "from google.colab import userdata\n",
    "\n",
    "# Get API key from Colab secrets\n",
    "# Go to: \\ud83d\\udd11 icon in left sidebar -> Add new secret\n",
    "# Name: ANTHROPIC_API_KEY, Value: your API key\n",
    "try:\n",
    "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
    "    print(\"\\u2713 API key loaded from Colab secrets\")\n",
    "except:\n",
    "    print(\"\\u26a0 No API key found in Colab secrets\")\n",
    "    print(\"Please add ANTHROPIC_API_KEY in the secrets panel (\\ud83d\\udd11 icon)\")\n",
    "    ANTHROPIC_API_KEY = input(\"Or enter API key here: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: PDF Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "class PDFProcessor:\n",
    "    \"\"\"Extract text from PDFs while preserving structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_path: str, source_type: str = \"ksp\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pdf_path: Path to PDF file\n",
    "            source_type: \"ksp\" or \"textbook\"\n",
    "        \"\"\"\n",
    "        self.pdf_path = pdf_path\n",
    "        self.source_type = source_type\n",
    "        self.filename = Path(pdf_path).stem\n",
    "        self.metadata = self._extract_metadata()\n",
    "    \n",
    "    def _extract_metadata(self) -> Dict:\n",
    "        \"\"\"Extract metadata from filename.\"\"\"\n",
    "        if self.source_type == \"ksp\":\n",
    "            # Expected format: YYYY_CCC_Title.pdf\n",
    "            # Example: 2015_VNM_Industrial_Policy.pdf\n",
    "            pattern = r'(\\d{4})_([A-Z]{3})_(.+)'\n",
    "            match = re.match(pattern, self.filename)\n",
    "            if match:\n",
    "                return {\n",
    "                    'source_type': 'ksp',\n",
    "                    'year': match.group(1),\n",
    "                    'country': match.group(2),\n",
    "                    'title': match.group(3).replace('_', ' '),\n",
    "                    'filename': self.filename\n",
    "                }\n",
    "        else:  # textbook\n",
    "            # Expected format: Author_Year.pdf\n",
    "            # Example: Perkins_2013.pdf\n",
    "            return {\n",
    "                'source_type': 'textbook',\n",
    "                'filename': self.filename\n",
    "            }\n",
    "        \n",
    "        return {'source_type': self.source_type, 'filename': self.filename}\n",
    "    \n",
    "    def extract_sections(self) -> List[Dict]:\n",
    "        \"\"\"Extract text organized by sections.\"\"\"\n",
    "        doc = fitz.open(self.pdf_path)\n",
    "        sections = []\n",
    "        current_section = {'title': 'Introduction', 'content': '', 'page': 1}\n",
    "        \n",
    "        for page_num, page in enumerate(doc, 1):\n",
    "            # Extract text blocks with font information\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            \n",
    "            for block in blocks:\n",
    "                if \"lines\" not in block:\n",
    "                    continue\n",
    "                \n",
    "                for line in block[\"lines\"]:\n",
    "                    text = \"\"\n",
    "                    font_size = 0\n",
    "                    \n",
    "                    for span in line[\"spans\"]:\n",
    "                        text += span[\"text\"]\n",
    "                        font_size = max(font_size, span[\"size\"])\n",
    "                    \n",
    "                    text = text.strip()\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    \n",
    "                    # Detect section headers (larger font, shorter text)\n",
    "                    if font_size > 12 and len(text) < 100:\n",
    "                        # Save previous section\n",
    "                        if current_section['content'].strip():\n",
    "                            sections.append(current_section)\n",
    "                        \n",
    "                        # Start new section\n",
    "                        current_section = {\n",
    "                            'title': text,\n",
    "                            'content': '',\n",
    "                            'page': page_num\n",
    "                        }\n",
    "                    else:\n",
    "                        current_section['content'] += text + \"\\n\"\n",
    "        \n",
    "        # Add final section\n",
    "        if current_section['content'].strip():\n",
    "            sections.append(current_section)\n",
    "        \n",
    "        doc.close()\n",
    "        return sections\n",
    "    \n",
    "    def extract_full_text(self) -> str:\n",
    "        \"\"\"Extract all text from PDF.\"\"\"\n",
    "        doc = fitz.open(self.pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        return text\n",
    "    \n",
    "    def get_page_count(self) -> int:\n",
    "        \"\"\"Get number of pages.\"\"\"\n",
    "        doc = fitz.open(self.pdf_path)\n",
    "        count = len(doc)\n",
    "        doc.close()\n",
    "        return count\n",
    "\n",
    "# Test PDF processing\n",
    "print(\"Testing PDF processor...\")\n",
    "# Note: This will only work after you upload PDFs\n",
    "# Uncomment and run after uploading files:\n",
    "\n",
    "# test_pdf = f\"{config.ksp_dir}/2015_VNM_Example.pdf\"  # Replace with your actual file\n",
    "# if os.path.exists(test_pdf):\n",
    "#     processor = PDFProcessor(test_pdf, source_type=\"ksp\")\n",
    "#     print(f\"\\u2713 Metadata: {processor.metadata}\")\n",
    "#     sections = processor.extract_sections()\n",
    "#     print(f\"\\u2713 Extracted {len(sections)} sections\")\n",
    "#     print(f\"\\u2713 Total pages: {processor.get_page_count()}\")\n",
    "# else:\n",
    "#     print(f\"Upload PDF to: {test_pdf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class SemanticChunker:\n",
    "    \"\"\"Create semantic chunks from extracted text.\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "            length_function=len\n",
    "        )\n",
    "    \n",
    "    def chunk_sections(self, sections: List[Dict], metadata: Dict) -> List[Dict]:\n",
    "        \"\"\"Create chunks from sections with metadata.\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for section_idx, section in enumerate(sections):\n",
    "            section_text = section['content']\n",
    "            section_title = section['title']\n",
    "            \n",
    "            # Split section into smaller chunks if needed\n",
    "            sub_chunks = self.splitter.split_text(section_text)\n",
    "            \n",
    "            for chunk_idx, chunk_text in enumerate(sub_chunks):\n",
    "                chunk = {\n",
    "                    'text': chunk_text,\n",
    "                    'metadata': {\n",
    "                        **metadata,  # Include all document metadata\n",
    "                        'section_title': section_title,\n",
    "                        'section_index': section_idx,\n",
    "                        'section_page': section['page'],\n",
    "                        'chunk_index': chunk_idx,\n",
    "                        'chunk_id': f\"{metadata['filename']}_s{section_idx}_c{chunk_idx}\"\n",
    "                    }\n",
    "                }\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Test chunker\n",
    "chunker_ksp = SemanticChunker(chunk_size=config.ksp_chunk_size)\n",
    "chunker_textbook = SemanticChunker(chunk_size=config.textbook_chunk_size)\n",
    "print(\"\\u2713 Chunkers initialized\")\n",
    "print(f\"  KSP chunk size: {config.ksp_chunk_size} characters\")\n",
    "print(f\"  Textbook chunk size: {config.textbook_chunk_size} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Embedding & Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manage embeddings and ChromaDB vector database.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str, persist_directory: str = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            collection_name: Name of the collection\n",
    "            persist_directory: Where to save the database (Google Drive)\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        print(f\"Loading embedding model: {config.embedding_model}...\")\n",
    "        self.embedding_model = SentenceTransformer(config.embedding_model)\n",
    "        print(\"\\u2713 Embedding model loaded\")\n",
    "        \n",
    "        # Initialize ChromaDB with persistence\n",
    "        if persist_directory is None:\n",
    "            persist_directory = config.vector_db_dir\n",
    "        \n",
    "        self.client = chromadb.Client(Settings(\n",
    "            persist_directory=persist_directory,\n",
    "            anonymized_telemetry=False\n",
    "        ))\n",
    "        \n",
    "        # Create or get collection\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(name=collection_name)\n",
    "            print(f\"\\u2713 Loaded existing collection: {collection_name}\")\n",
    "        except:\n",
    "            self.collection = self.client.create_collection(\n",
    "                name=collection_name,\n",
    "                metadata={\"description\": f\"Collection for {collection_name}\"}\n",
    "            )\n",
    "            print(f\"\\u2713 Created new collection: {collection_name}\")\n",
    "    \n",
    "    def add_documents(self, chunks: List[Dict], batch_size: int = 32):\n",
    "        \"\"\"Add document chunks to vector database.\"\"\"\n",
    "        documents = [chunk['text'] for chunk in chunks]\n",
    "        metadatas = [chunk['metadata'] for chunk in chunks]\n",
    "        ids = [chunk['metadata']['chunk_id'] for chunk in chunks]\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to {self.collection_name}...\")\n",
    "        \n",
    "        # Create embeddings in batches\n",
    "        all_embeddings = []\n",
    "        for i in tqdm(range(0, len(documents), batch_size), desc=\"Embedding\"):\n",
    "            batch_docs = documents[i:i+batch_size]\n",
    "            embeddings = self.embedding_model.encode(\n",
    "                batch_docs,\n",
    "                show_progress_bar=False,\n",
    "                convert_to_numpy=True\n",
    "            ).tolist()\n",
    "            all_embeddings.extend(embeddings)\n",
    "        \n",
    "        # Add to ChromaDB in batches\n",
    "        for i in tqdm(range(0, len(documents), batch_size), desc=\"Storing\"):\n",
    "            batch_end = min(i + batch_size, len(documents))\n",
    "            \n",
    "            self.collection.add(\n",
    "                documents=documents[i:batch_end],\n",
    "                embeddings=all_embeddings[i:batch_end],\n",
    "                metadatas=metadatas[i:batch_end],\n",
    "                ids=ids[i:batch_end]\n",
    "            )\n",
    "        \n",
    "        print(f\"\\u2713 Added {len(documents)} chunks to collection\")\n",
    "    \n",
    "    def search(self, \n",
    "               query: str, \n",
    "               n_results: int = 5,\n",
    "               filter_dict: Dict = None) -> Dict:\n",
    "        \"\"\"Search for relevant chunks.\"\"\"\n",
    "        # Create query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])[0].tolist()\n",
    "        \n",
    "        # Search\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            where=filter_dict\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get collection statistics.\"\"\"\n",
    "        count = self.collection.count()\n",
    "        return {\n",
    "            'collection_name': self.collection_name,\n",
    "            'total_chunks': count\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Dual Vector Store Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector stores\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INITIALIZING VECTOR DATABASES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ksp_store = VectorStore(\n",
    "    collection_name=config.ksp_collection,\n",
    "    persist_directory=config.vector_db_dir\n",
    ")\n",
    "\n",
    "textbook_store = VectorStore(\n",
    "    collection_name=config.textbook_collection,\n",
    "    persist_directory=config.vector_db_dir\n",
    ")\n",
    "\n",
    "print(\"\\n\\u2713 Vector stores initialized\")\n",
    "print(f\"  ChromaDB persisted to: {config.vector_db_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Document Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_index_ksp_reports():\n",
    "    \"\"\"Process all KSP reports and add to vector database.\"\"\"\n",
    "    ksp_dir = Path(config.ksp_dir)\n",
    "    pdf_files = list(ksp_dir.glob(\"*.pdf\"))\n",
    "    \n",
    "    print(f\"\\nFound {len(pdf_files)} KSP reports\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_path.name}\")\n",
    "        \n",
    "        # Extract text\n",
    "        processor = PDFProcessor(str(pdf_path), source_type=\"ksp\")\n",
    "        sections = processor.extract_sections()\n",
    "        print(f\"  Extracted {len(sections)} sections\")\n",
    "        \n",
    "        # Create chunks\n",
    "        chunks = chunker_ksp.chunk_sections(sections, processor.metadata)\n",
    "        print(f\"  Created {len(chunks)} chunks\")\n",
    "        \n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    # Add to vector database\n",
    "    if all_chunks:\n",
    "        ksp_store.add_documents(all_chunks)\n",
    "        \n",
    "        # Save processed data\n",
    "        output_path = f\"{config.processed_dir}/ksp_chunks.json\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(all_chunks, f, indent=2)\n",
    "        print(f\"\\n\\u2713 Saved processed chunks to: {output_path}\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def process_and_index_textbooks():\n",
    "    \"\"\"Process textbooks and add to vector database.\"\"\"\n",
    "    textbook_dir = Path(config.textbook_dir)\n",
    "    pdf_files = list(textbook_dir.glob(\"*.pdf\"))\n",
    "    \n",
    "    print(f\"\\nFound {len(pdf_files)} textbooks\")\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_path.name}\")\n",
    "        \n",
    "        # Extract text\n",
    "        processor = PDFProcessor(str(pdf_path), source_type=\"textbook\")\n",
    "        sections = processor.extract_sections()\n",
    "        print(f\"  Extracted {len(sections)} sections\")\n",
    "        \n",
    "        # Create chunks (larger for textbooks)\n",
    "        chunks = chunker_textbook.chunk_sections(sections, processor.metadata)\n",
    "        print(f\"  Created {len(chunks)} chunks\")\n",
    "        \n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    # Add to vector database\n",
    "    if all_chunks:\n",
    "        textbook_store.add_documents(all_chunks)\n",
    "        \n",
    "        # Save processed data\n",
    "        output_path = f\"{config.processed_dir}/textbook_chunks.json\"\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(all_chunks, f, indent=2)\n",
    "        print(f\"\\n\\u2713 Saved processed chunks to: {output_path}\")\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Document Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: DOCUMENT INDEXING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if PDFs are uploaded\n",
    "ksp_pdfs = list(Path(config.ksp_dir).glob(\"*.pdf\"))\n",
    "textbook_pdfs = list(Path(config.textbook_dir).glob(\"*.pdf\"))\n",
    "\n",
    "if len(ksp_pdfs) == 0:\n",
    "    print(f\"\\n\\u26a0 No KSP reports found in: {config.ksp_dir}\")\n",
    "    print(\"Please upload 4 KSP PDF reports to this directory\")\n",
    "    print(\"\\nTo upload:\")\n",
    "    print(\"1. Click the folder icon in the left sidebar\")\n",
    "    print(f\"2. Navigate to: {config.ksp_dir}\")\n",
    "    print(\"3. Right-click -> Upload\")\n",
    "else:\n",
    "    print(f\"\\n\\u2713 Found {len(ksp_pdfs)} KSP reports\")\n",
    "    ksp_chunks = process_and_index_ksp_reports()\n",
    "\n",
    "if len(textbook_pdfs) == 0:\n",
    "    print(f\"\\n\\u26a0 No textbooks found in: {config.textbook_dir}\")\n",
    "    print(\"Please upload 2 textbook PDFs to this directory\")\n",
    "else:\n",
    "    print(f\"\\n\\u2713 Found {len(textbook_pdfs)} textbooks\")\n",
    "    textbook_chunks = process_and_index_textbooks()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INDEXING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKSP Collection: {ksp_store.get_stats()}\")\n",
    "print(f\"Textbook Collection: {textbook_store.get_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: LLM Extraction Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "class PolicyExtractor:\n",
    "    \"\"\"Extract structured policy information using Claude API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.model = config.llm_model\n",
    "        \n",
    "        self.extraction_prompt = \"\"\"You are analyzing development policy documents to extract structured information.\n",
    "\n",
    "CONTEXT FROM KSP REPORT:\n",
    "{ksp_context}\n",
    "\n",
    "RELATED THEORETICAL CONCEPTS (from textbooks):\n",
    "{theory_context}\n",
    "\n",
    "TASK:\n",
    "Extract ALL policies, programs, or initiatives mentioned in the KSP context.\n",
    "\n",
    "For EACH policy/program, provide:\n",
    "1. policy_name: Official title or clear description\n",
    "2. year_initiated: Year it started (null if not mentioned)\n",
    "3. organization: Responsible government ministry/agency (null if not mentioned)\n",
    "4. challenge_addressed: What development problem did it address?\n",
    "5. policy_instruments: List of specific mechanisms/tools used (e.g., [\"Tax incentives\", \"Credit allocation\"])\n",
    "6. sector: Economic sector (e.g., \"Manufacturing\", \"Finance\", \"Agriculture\")\n",
    "7. development_stage: \"early_industrialization\", \"middle_income\", or \"advanced\" (null if unclear)\n",
    "8. evidence_quote: Direct quote from KSP document supporting this (REQUIRED - must be verbatim from context)\n",
    "9. related_theory: Which theoretical concept from the textbook context relates to this policy? (null if none)\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return ONLY a valid JSON array. No markdown, no preamble, no explanation.\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"policy_name\": \"string\",\n",
    "    \"year_initiated\": integer or null,\n",
    "    \"organization\": \"string\" or null,\n",
    "    \"challenge_addressed\": \"string\",\n",
    "    \"policy_instruments\": [\"string\", \"string\"],\n",
    "    \"sector\": \"string\",\n",
    "    \"development_stage\": \"early_industrialization\" | \"middle_income\" | \"advanced\" | null,\n",
    "    \"evidence_quote\": \"string from KSP context\",\n",
    "    \"related_theory\": \"string from textbook context\" or null\n",
    "  }}\n",
    "]\n",
    "\n",
    "CRITICAL RULES:\n",
    "- Only extract information explicitly stated in the KSP context\n",
    "- Every policy MUST have an evidence_quote from the KSP context\n",
    "- If information not mentioned, use null\n",
    "- Extract ALL policies mentioned, not just major ones\n",
    "- Return valid JSON only\n",
    "- If no policies found, return: []\"\"\"\n",
    "    \n",
    "    def extract_from_contexts(self, \n",
    "                              ksp_context: str, \n",
    "                              theory_context: str = \"\") -> List[Dict]:\n",
    "        \"\"\"Extract policies from KSP context with theory linking.\"\"\"\n",
    "        \n",
    "        prompt = self.extraction_prompt.format(\n",
    "            ksp_context=ksp_context,\n",
    "            theory_context=theory_context if theory_context else \"No theoretical context provided\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=config.max_tokens,\n",
    "                temperature=config.temperature,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            # Extract text from response\n",
    "            content = response.content[0].text\n",
    "            \n",
    "            # Clean response (remove markdown if present)\n",
    "            content = content.strip()\n",
    "            if content.startswith('```json'):\n",
    "                content = content[7:]\n",
    "            if content.startswith('```'):\n",
    "                content = content[3:]\n",
    "            if content.endswith('```'):\n",
    "                content = content[:-3]\n",
    "            content = content.strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            policies = json.loads(content)\n",
    "            \n",
    "            if not isinstance(policies, list):\n",
    "                policies = [policies]\n",
    "            \n",
    "            return policies\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON parsing error: {e}\")\n",
    "            print(f\"Response: {content[:200]}...\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Extraction error: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = PolicyExtractor(api_key=ANTHROPIC_API_KEY)\n",
    "print(\"\\u2713 Policy extractor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policies_from_report(report_filename: str) -> List[Dict]:\n",
    "    \"\"\"Extract policies from a single KSP report with theory linking.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTING FROM: {report_filename}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Step 1: Query KSP collection for policy-relevant passages\n",
    "    ksp_queries = [\n",
    "        \"Korean government policy program initiative implementation\",\n",
    "        \"ministry organization agency institution\",\n",
    "        \"economic development challenge problem solution\",\n",
    "        \"policy instrument mechanism tool intervention\"\n",
    "    ]\n",
    "    \n",
    "    all_ksp_context = set()\n",
    "    for query in ksp_queries:\n",
    "        results = ksp_store.search(\n",
    "            query=query,\n",
    "            n_results=config.ksp_top_k,\n",
    "            filter_dict={'filename': report_filename}\n",
    "        )\n",
    "        if results['documents'][0]:\n",
    "            all_ksp_context.update(results['documents'][0])\n",
    "    \n",
    "    ksp_context = \"\\n\\n---\\n\\n\".join(all_ksp_context)\n",
    "    print(f\"Retrieved {len(all_ksp_context)} unique KSP chunks\")\n",
    "    \n",
    "    # Step 2: Query textbook collection for related theory\n",
    "    theory_query = f\"development policy economic growth industrial policy\"\n",
    "    theory_results = textbook_store.search(\n",
    "        query=theory_query,\n",
    "        n_results=config.textbook_top_k\n",
    "    )\n",
    "    \n",
    "    theory_context = \"\\n\\n---\\n\\n\".join(theory_results['documents'][0]) if theory_results['documents'][0] else \"\"\n",
    "    print(f\"Retrieved {len(theory_results['documents'][0])} textbook chunks\")\n",
    "    \n",
    "    # Step 3: Extract policies with theory linking\n",
    "    print(\"\\nExtracting policies with Claude API...\")\n",
    "    policies = extractor.extract_from_contexts(ksp_context, theory_context)\n",
    "    \n",
    "    # Add source metadata\n",
    "    for policy in policies:\n",
    "        policy['source_report'] = report_filename\n",
    "    \n",
    "    print(f\"\\u2713 Extracted {len(policies)} policies\")\n",
    "    \n",
    "    return policies\n",
    "\n",
    "def extract_all_policies() -> List[Dict]:\n",
    "    \"\"\"Extract policies from all KSP reports.\"\"\"\n",
    "    ksp_pdfs = list(Path(config.ksp_dir).glob(\"*.pdf\"))\n",
    "    \n",
    "    all_policies = []\n",
    "    \n",
    "    for pdf_path in ksp_pdfs:\n",
    "        report_filename = pdf_path.stem\n",
    "        policies = extract_policies_from_report(report_filename)\n",
    "        all_policies.extend(policies)\n",
    "    \n",
    "    # Save results\n",
    "    output_path = f\"{config.results_dir}/extracted_policies.json\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(all_policies, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXTRACTION COMPLETE\")\n",
    "    print('='*60)\n",
    "    print(f\"Total policies extracted: {len(all_policies)}\")\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "    \n",
    "    return all_policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Run Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: POLICY EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if we have indexed documents\n",
    "if ksp_store.get_stats()['total_chunks'] == 0:\n",
    "    print(\"\\n\\u26a0 No documents indexed yet!\")\n",
    "    print(\"Please run the indexing section (Section 7) first\")\n",
    "else:\n",
    "    # Extract policies\n",
    "    extracted_policies = extract_all_policies()\n",
    "    \n",
    "    # Display sample\n",
    "    if extracted_policies:\n",
    "        print(\"\\nSample extracted policy:\")\n",
    "        print(json.dumps(extracted_policies[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 11: Evaluation (if gold standard exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluate extraction quality against gold standard.\"\"\"\n",
    "    \n",
    "    def __init__(self, gold_standard_path: str):\n",
    "        with open(gold_standard_path) as f:\n",
    "            self.gold_standard = json.load(f)\n",
    "    \n",
    "    def evaluate(self, predictions: List[Dict]) -> Dict:\n",
    "        \"\"\"Evaluate predictions against gold standard.\"\"\"\n",
    "        \n",
    "        # Group by report\n",
    "        pred_by_report = {}\n",
    "        for p in predictions:\n",
    "            report = p.get('source_report', 'unknown')\n",
    "            if report not in pred_by_report:\n",
    "                pred_by_report[report] = []\n",
    "            pred_by_report[report].append(p)\n",
    "        \n",
    "        gold_by_report = {}\n",
    "        for g in self.gold_standard:\n",
    "            report = g.get('source_report', 'unknown')\n",
    "            if report not in gold_by_report:\n",
    "                gold_by_report[report] = []\n",
    "            gold_by_report[report].append(g)\n",
    "        \n",
    "        # Calculate metrics for each report\n",
    "        results = []\n",
    "        for report in set(list(pred_by_report.keys()) + list(gold_by_report.keys())):\n",
    "            preds = pred_by_report.get(report, [])\n",
    "            golds = gold_by_report.get(report, [])\n",
    "            \n",
    "            # Entity-level evaluation (policy names)\n",
    "            pred_names = set(p['policy_name'].lower() for p in preds)\n",
    "            gold_names = set(g['policy_name'].lower() for g in golds)\n",
    "            \n",
    "            tp = len(pred_names & gold_names)\n",
    "            fp = len(pred_names - gold_names)\n",
    "            fn = len(gold_names - pred_names)\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'report': report,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'num_predicted': len(preds),\n",
    "                'num_gold': len(golds)\n",
    "            })\n",
    "        \n",
    "        # Calculate overall metrics\n",
    "        total_tp = sum(r['tp'] for r in results)\n",
    "        total_fp = sum(r['fp'] for r in results)\n",
    "        total_fn = sum(r['fn'] for r in results)\n",
    "        \n",
    "        overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'overall': {\n",
    "                'precision': overall_precision,\n",
    "                'recall': overall_recall,\n",
    "                'f1': overall_f1,\n",
    "                'total_predicted': sum(r['num_predicted'] for r in results),\n",
    "                'total_gold': sum(r['num_gold'] for r in results)\n",
    "            },\n",
    "            'by_report': results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if gold standard exists\n",
    "gold_standard_path = f\"{config.project_dir}/data/gold_standard/annotations.json\"\n",
    "if os.path.exists(gold_standard_path):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    evaluator = Evaluator(gold_standard_path)\n",
    "    eval_results = evaluator.evaluate(extracted_policies)\n",
    "    \n",
    "    print(\"\\nOverall Performance:\")\n",
    "    print(f\"  Precision: {eval_results['overall']['precision']:.3f}\")\n",
    "    print(f\"  Recall: {eval_results['overall']['recall']:.3f}\")\n",
    "    print(f\"  F1 Score: {eval_results['overall']['f1']:.3f}\")\n",
    "    \n",
    "    # Save evaluation results\n",
    "    eval_output = f\"{config.results_dir}/evaluation_results.json\"\n",
    "    with open(eval_output, 'w') as f:\n",
    "        json.dump(eval_results, f, indent=2)\n",
    "    print(f\"\\n\\u2713 Evaluation results saved to: {eval_output}\")\n",
    "else:\n",
    "    print(f\"\\n\\u26a0 No gold standard found at: {gold_standard_path}\")\n",
    "    print(\"Create gold standard annotations to enable evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 12: Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_policies(policies: List[Dict]):\n",
    "    \"\"\"Analyze extracted policies.\"\"\"\n",
    "    \n",
    "    if not policies:\n",
    "        print(\"No policies to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"POLICY ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df = pd.DataFrame(policies)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nTotal policies extracted: {len(policies)}\")\n",
    "    print(f\"Unique reports: {df['source_report'].nunique()}\")\n",
    "    \n",
    "    # Sector distribution\n",
    "    print(\"\\nPolicies by Sector:\")\n",
    "    sector_counts = df['sector'].value_counts()\n",
    "    print(sector_counts)\n",
    "    \n",
    "    # Development stage distribution\n",
    "    if 'development_stage' in df.columns:\n",
    "        print(\"\\nPolicies by Development Stage:\")\n",
    "        stage_counts = df['development_stage'].value_counts()\n",
    "        print(stage_counts)\n",
    "    \n",
    "    # Theory linking\n",
    "    if 'related_theory' in df.columns:\n",
    "        theory_linked = df['related_theory'].notna().sum()\n",
    "        print(f\"\\nPolicies linked to theory: {theory_linked} ({theory_linked/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualization 1: Sector distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sector_counts.plot(kind='barh')\n",
    "    plt.title('Policies by Sector')\n",
    "    plt.xlabel('Number of Policies')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.results_dir}/sector_distribution.png\", dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"\\n\\u2713 Saved sector distribution chart\")\n",
    "    \n",
    "    # Visualization 2: Theory-Practice Network\n",
    "    if 'related_theory' in df.columns:\n",
    "        create_theory_practice_network(policies)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_theory_practice_network(policies: List[Dict]):\n",
    "    \"\"\"Create network graph of theory-practice connections.\"\"\"\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for policy in policies:\n",
    "        policy_name = policy['policy_name'][:30]  # Truncate for readability\n",
    "        G.add_node(policy_name, node_type='policy', sector=policy.get('sector', 'Unknown'))\n",
    "        \n",
    "        if policy.get('related_theory'):\n",
    "            theory = policy['related_theory'][:30]\n",
    "            G.add_node(theory, node_type='theory')\n",
    "            G.add_edge(policy_name, theory)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "    \n",
    "    # Color nodes by type\n",
    "    colors = ['lightblue' if G.nodes[n].get('node_type') == 'policy' else 'lightcoral' for n in G.nodes()]\n",
    "    \n",
    "    nx.draw(G, pos, \n",
    "            node_color=colors,\n",
    "            with_labels=True,\n",
    "            font_size=8,\n",
    "            node_size=500,\n",
    "            alpha=0.7)\n",
    "    \n",
    "    plt.title('Theory-Practice Network\\n(Blue=Policies, Red=Theories)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.results_dir}/theory_practice_network.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\u2713 Saved theory-practice network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "if 'extracted_policies' in locals() and extracted_policies:\n",
    "    policy_df = analyze_policies(extracted_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 13: Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PILOT STUDY COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nResults saved in: {config.project_dir}\")\n",
    "print(\"\\nKey outputs:\")\n",
    "print(f\"  1. Processed chunks: {config.processed_dir}/\")\n",
    "print(f\"  2. Vector database: {config.vector_db_dir}/\")\n",
    "print(f\"  3. Extracted policies: {config.results_dir}/extracted_policies.json\")\n",
    "print(f\"  4. Visualizations: {config.results_dir}/*.png\")\n",
    "\n",
    "if 'eval_results' in locals():\n",
    "    print(f\"\\n  Overall F1 Score: {eval_results['overall']['f1']:.3f}\")\n",
    "    if eval_results['overall']['f1'] >= 0.70:\n",
    "        print(\"  \\u2713 Pilot successful! Ready to scale to full 566 reports\")\n",
    "    else:\n",
    "        print(\"  \\u26a0 Consider refining prompts or chunking strategy\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Review extracted policies for quality\")\n",
    "print(\"  2. Create gold standard annotations if not done\")\n",
    "print(\"  3. Refine extraction prompts based on errors\")\n",
    "print(\"  4. Scale to full dataset (566 reports)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
