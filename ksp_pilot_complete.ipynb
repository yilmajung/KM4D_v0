{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/yilmajung/KM4D_v0/blob/main/ksp_pilot_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KSP Knowledge Extraction - Chapter-Level Classification & Policy Extraction\n",
    "\n",
    "**Pipeline Overview:**\n",
    "For each chapter/sub-chapter of a KSP report, this notebook performs:\n",
    "1. **Taxonomy Classification** — tags sectors & keywords from the development cooperation taxonomy\n",
    "2. **Knowledge Type** — classifies as one of 4 knowledge types\n",
    "3. **Korean Policy Extraction** — extracts policy_name, year_initiated, organization, challenge_addressed, policy_instruments, sector\n",
    "4. **Theory Linking** — matches related theories from development economics textbooks (via RAG)\n",
    "\n",
    "**Key Design:**\n",
    "- Chapters are read directly (no KSP chunking/RAG needed)\n",
    "- Textbook RAG is kept for theory linking\n",
    "- Single combined LLM call per chapter\n",
    "- Full taxonomy embedded in prompt (~3K tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pymupdf pdfplumber sentence-transformers chromadb anthropic pandas numpy scikit-learn matplotlib seaborn plotly networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for file persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create project directory in Google Drive\n",
    "import os\n",
    "project_dir = '/content/drive/MyDrive/KM4D_v0'\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/raw/ksp_reports', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/raw/textbooks', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/processed', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data/results', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/vector_db', exist_ok=True)\n",
    "\n",
    "print('\\u2713 Project directory created in Google Drive')\n",
    "print(f'  Location: {project_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the chapter-level analysis pipeline.\"\"\"\n",
    "\n",
    "    # Directories\n",
    "    project_dir: str = project_dir\n",
    "    ksp_dir: str = f'{project_dir}/data/raw/ksp_reports'\n",
    "    textbook_dir: str = f'{project_dir}/data/raw/textbooks'\n",
    "    processed_dir: str = f'{project_dir}/data/processed'\n",
    "    results_dir: str = f'{project_dir}/data/results'\n",
    "    vector_db_dir: str = f'{project_dir}/vector_db'\n",
    "\n",
    "    # Textbook chunking parameters\n",
    "    textbook_chunk_size: int = 768\n",
    "    chunk_overlap: int = 50\n",
    "\n",
    "    # Embedding model\n",
    "    embedding_model: str = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "    # LLM API\n",
    "    llm_model: str = 'claude-sonnet-4-20250514'\n",
    "    temperature: float = 0.1\n",
    "    max_tokens: int = 4096\n",
    "\n",
    "    # Retrieval parameters (textbook only)\n",
    "    textbook_top_k: int = 3\n",
    "\n",
    "    # Textbook collection\n",
    "    textbook_collection: str = 'textbooks_pilot'\n",
    "\n",
    "config = Config()\n",
    "print('\\u2713 Config initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Key Setup\n",
    "from google.colab import userdata\n",
    "\n",
    "try:\n",
    "    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
    "    print('\\u2713 API key loaded from Colab secrets')\n",
    "except:\n",
    "    print('\\u26a0 No API key found in Colab secrets')\n",
    "    print('Please add ANTHROPIC_API_KEY in the secrets panel (\\ud83d\\udd11 icon)')\n",
    "    ANTHROPIC_API_KEY = input('Or enter API key here: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Taxonomy Reference & Knowledge Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full 6-sector, 4-level taxonomy from the Development Cooperation Taxonomy\n",
    "# Structure: Sector -> Sub-sector L1 -> Sub-sector L2 -> Keywords\n",
    "\n",
    "TAXONOMY = {\n",
    "    '(1) Economic Policy': {\n",
    "        'Macroeconomic Policy & Stability': {\n",
    "            'Fiscal Policy': [\n",
    "                'Public Finance Management (PFM)',\n",
    "                'Tax Policy & Administration',\n",
    "                'Public Expenditure Management',\n",
    "                'Sovereign Debt Management',\n",
    "                'Domestic Revenue Mobilization'\n",
    "            ],\n",
    "            'Monetary & Financial Policy': [\n",
    "                'Financial Sector Development',\n",
    "                'Monetary Policy & Central Banking',\n",
    "                'Financial Stability & Regulation',\n",
    "                'Financial Inclusion'\n",
    "            ]\n",
    "        },\n",
    "        'Inclusive & Sustainable Growth': {\n",
    "            'Growth Diagnostics & Strategy': [\n",
    "                'Economic Growth Strategy',\n",
    "                'Structural Reform',\n",
    "                'Productivity Analysis',\n",
    "                'Green Growth'\n",
    "            ],\n",
    "            'Poverty & Inequality': [\n",
    "                'Poverty Reduction Strategy',\n",
    "                'Inequality Analysis (Income, Wealth)',\n",
    "                'Social Protection Systems'\n",
    "            ]\n",
    "        },\n",
    "        'Investment & Private Sector Dev.': {\n",
    "            'Investment Climate': [\n",
    "                'Investment Policy & Promotion',\n",
    "                'Business Environment Reform',\n",
    "                'Competition Policy',\n",
    "                'Corporate Governance'\n",
    "            ],\n",
    "            'Private Sector Support': [\n",
    "                'Small & Medium Enterprise (SME) Dev.',\n",
    "                'Foreign Direct Investment (FDI)',\n",
    "                'Public-Private Partnerships (PPPs)',\n",
    "                'Entrepreneurship'\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    '(2) Social Services': {\n",
    "        'Education': {\n",
    "            'Education Policy & Systems': [\n",
    "                'Education Policy & Administration',\n",
    "                'Education Sector Planning',\n",
    "                'Teacher Training',\n",
    "                'Educational Facilities'\n",
    "            ],\n",
    "            'Levels of Education': [\n",
    "                'Early Childhood Education',\n",
    "                'Primary Education',\n",
    "                'Secondary Education',\n",
    "                'Higher Education',\n",
    "                'Vocational Training (TVET)',\n",
    "                'Adult Education & Lifelong Learning'\n",
    "            ]\n",
    "        },\n",
    "        'Health': {\n",
    "            'Health Policy & Systems': [\n",
    "                'Health Policy & Administration',\n",
    "                'Health Systems Strengthening',\n",
    "                'Health Financing & Insurance',\n",
    "                'Health Workforce Development',\n",
    "                'Digital Health'\n",
    "            ],\n",
    "            'Health Services & Outcomes': [\n",
    "                'Basic Health Care',\n",
    "                'Maternal & Child Health',\n",
    "                'Infectious Disease Control (HIV/AIDS, TB)',\n",
    "                'Non-Communicable Diseases (NCDs)',\n",
    "                'Mental Health',\n",
    "                'Pandemic Preparedness & Response'\n",
    "            ]\n",
    "        },\n",
    "        'Social Protection & Inclusion': {\n",
    "            'Social Welfare Services': [\n",
    "                'Social/Welfare Services',\n",
    "                'Child Protection',\n",
    "                'Services for the Elderly & Disabled'\n",
    "            ]\n",
    "        },\n",
    "        'Cross-Cutting Social Issues': {\n",
    "            'Gender Equality & Empowerment': [\n",
    "                'Gender Equality',\n",
    "                \"Women's Economic Empowerment\",\n",
    "                'Ending Violence Against Women & Girls',\n",
    "                'Youth Development & Employment',\n",
    "                'Food Security & Nutrition'\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    '(3) Digital Innovation': {\n",
    "        'Digital Policy & Governance': {\n",
    "            'Digital Strategy & Regulation': [\n",
    "                'National Digital Strategy',\n",
    "                'Data Governance & Privacy',\n",
    "                'Cybersecurity Policy',\n",
    "                'AI Governance & Ethics',\n",
    "                'Digital Taxation'\n",
    "            ]\n",
    "        },\n",
    "        'Digital Infrastructure': {\n",
    "            'Connectivity': [\n",
    "                'Universal Connectivity',\n",
    "                'Broadband Infrastructure',\n",
    "                'Mobile Networks (5G)'\n",
    "            ],\n",
    "            'Core Digital Systems': [\n",
    "                'Digital Public Infrastructure (DPI)',\n",
    "                'Digital Identity Systems',\n",
    "                'Digital Payment Systems'\n",
    "            ]\n",
    "        },\n",
    "        'Digital Transformation': {\n",
    "            'Digital Inclusion & Skills': [\n",
    "                'Digital Literacy & Skills',\n",
    "                'Addressing the Digital Divide',\n",
    "                'Gender and Digital Inclusion'\n",
    "            ],\n",
    "            'Digital Economy & Services': [\n",
    "                'E-Commerce',\n",
    "                'Digital Financial Services (FinTech)',\n",
    "                'E-Government Services',\n",
    "                'Digital Entrepreneurship',\n",
    "                'Platform Economy'\n",
    "            ]\n",
    "        },\n",
    "        'Emerging Technologies': {\n",
    "            'Technology Adoption': [\n",
    "                'Artificial Intelligence (AI)',\n",
    "                'Internet of Things (IoT)',\n",
    "                'Blockchain',\n",
    "                'Big Data Analytics'\n",
    "            ],\n",
    "            'Digital Public Goods': [\n",
    "                'Open Source Software & Data',\n",
    "                'Open Standards'\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    '(4) Production & Trade': {\n",
    "        'Agriculture, Forestry & Fisheries': {\n",
    "            'Agricultural Development': [\n",
    "                'Agricultural Policy & Administration',\n",
    "                'Agricultural Research & Extension',\n",
    "                'Sustainable Agriculture',\n",
    "                'Agricultural Value Chains',\n",
    "                'Smallholder Farmer Support'\n",
    "            ],\n",
    "            'Forestry & Fisheries': [\n",
    "                'Forestry Development',\n",
    "                'Sustainable Forest Management',\n",
    "                'Fisheries Development',\n",
    "                'Aquaculture'\n",
    "            ]\n",
    "        },\n",
    "        'Industry & Services': {\n",
    "            'Industrial Development': [\n",
    "                'Industrial Policy',\n",
    "                'Manufacturing',\n",
    "                'Agro-industry',\n",
    "                'Mineral Resources & Mining'\n",
    "            ],\n",
    "            'Services Sector Development': [\n",
    "                'Tourism Development',\n",
    "                'Business & Other Services'\n",
    "            ]\n",
    "        },\n",
    "        'Trade Policy & Facilitation': {\n",
    "            'Trade Policy & Regulation': [\n",
    "                'Trade Policy & Regulations',\n",
    "                'WTO Accession & Compliance',\n",
    "                'Regional Trade Agreements'\n",
    "            ],\n",
    "            'Trade Facilitation': [\n",
    "                'Customs Modernization',\n",
    "                'Border Management',\n",
    "                'Aid for Trade',\n",
    "                'Global Value Chains (GVCs)'\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    '(5) Infrastructure': {\n",
    "        'Infrastructure Policy & Finance': {\n",
    "            'Infrastructure Governance': [\n",
    "                'Infrastructure Policy & Planning',\n",
    "                'Infrastructure Governance',\n",
    "                'Public Investment Management'\n",
    "            ],\n",
    "            'Infrastructure Finance': [\n",
    "                'Infrastructure Financing',\n",
    "                'Private Investment in Infrastructure',\n",
    "                'Blended Finance'\n",
    "            ]\n",
    "        },\n",
    "        'Transport': {\n",
    "            'Transport Policy & Planning': [\n",
    "                'Transport Policy & Administration',\n",
    "                'Urban Transport',\n",
    "                'Rural Transport'\n",
    "            ],\n",
    "            'Transport Modalities': [\n",
    "                'Road Transport',\n",
    "                'Rail Transport',\n",
    "                'Water Transport (Ports, Inland)',\n",
    "                'Air Transport'\n",
    "            ]\n",
    "        },\n",
    "        'Water & Sanitation': {\n",
    "            'Water & Sanitation Policy': [\n",
    "                'Water & Sanitation Policy',\n",
    "                'Integrated Water Resource Management'\n",
    "            ],\n",
    "            'Water & Sanitation Services': [\n",
    "                'Water Supply Systems',\n",
    "                'Sanitation & Hygiene (WASH)',\n",
    "                'Wastewater Management'\n",
    "            ]\n",
    "        },\n",
    "        'Urban & Rural Development': {\n",
    "            'Urban Development': [\n",
    "                'Urban Development & Management',\n",
    "                'Affordable Housing'\n",
    "            ],\n",
    "            'Rural Development': [\n",
    "                'Rural Development'\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    '(6) Energy & Environment': {\n",
    "        'Environmental Policy & Management': {\n",
    "            'Environmental Governance': [\n",
    "                'Environmental Policy & Admin. Management',\n",
    "                'Biosafety & Biodiversity',\n",
    "                'Water Resources Protection',\n",
    "                'Circular Economy'\n",
    "            ]\n",
    "        },\n",
    "        'Climate Change': {\n",
    "            'Climate Change Mitigation': [\n",
    "                'Renewable Energy Generation',\n",
    "                'Energy Efficiency',\n",
    "                'Greenhouse Gas (GHG) Reduction',\n",
    "                'Carbon Markets & Pricing'\n",
    "            ],\n",
    "            'Climate Change Adaptation': [\n",
    "                'Climate Adaptation Strategies',\n",
    "                'Disaster Risk Reduction (DRR)',\n",
    "                'Climate Resilient Infrastructure',\n",
    "                'Climate Finance'\n",
    "            ]\n",
    "        },\n",
    "        'Energy': {\n",
    "            'Energy Policy & Access': [\n",
    "                'Energy Policy & Planning',\n",
    "                'Universal Energy Access',\n",
    "                'Just Energy Transition (JET)',\n",
    "                'Energy Sector Reform & Regulation'\n",
    "            ],\n",
    "            'Energy Generation & Supply': [\n",
    "                'Renewable Energy (Solar, Wind, Hydro)',\n",
    "                'Non-Renewable Energy (Fossil Fuels)',\n",
    "                'Energy Transmission & Distribution'\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Knowledge types for classifying chapter content\n",
    "KNOWLEDGE_TYPES = [\n",
    "    'Contextual background and situation analysis',\n",
    "    'Policy implementation and coordinating mechanism',\n",
    "    'Technical methodology and analytical framework',\n",
    "    'Recommendations and future directions'\n",
    "]\n",
    "\n",
    "\n",
    "def taxonomy_to_prompt_string() -> str:\n",
    "    \"\"\"Convert TAXONOMY dict to a readable string for LLM prompts.\"\"\"\n",
    "    lines = []\n",
    "    for sector, l1_dict in TAXONOMY.items():\n",
    "        lines.append(f'\\n{sector}')\n",
    "        for l1, l2_dict in l1_dict.items():\n",
    "            lines.append(f'  {l1}')\n",
    "            for l2, keywords in l2_dict.items():\n",
    "                lines.append(f'    {l2}')\n",
    "                for kw in keywords:\n",
    "                    lines.append(f'      - {kw}')\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def get_all_valid_keywords() -> set:\n",
    "    \"\"\"Get flat set of all valid keywords for validation.\"\"\"\n",
    "    keywords = set()\n",
    "    for sector, l1_dict in TAXONOMY.items():\n",
    "        for l1, l2_dict in l1_dict.items():\n",
    "            for l2, kw_list in l2_dict.items():\n",
    "                keywords.update(kw_list)\n",
    "    return keywords\n",
    "\n",
    "\n",
    "all_keywords = get_all_valid_keywords()\n",
    "print(f'\\u2713 Taxonomy loaded: {len(TAXONOMY)} sectors, {len(all_keywords)} keywords')\n",
    "print(f'\\u2713 Knowledge types: {len(KNOWLEDGE_TYPES)}')\n",
    "print()\n",
    "for sector in TAXONOMY:\n",
    "    n_kw = sum(len(kw) for l1 in TAXONOMY[sector].values() for kw in l1.values())\n",
    "    print(f'  {sector}: {n_kw} keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Chapter Extraction from KSP Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class ChapterExtractor:\n",
    "    \"\"\"Extract chapters and sub-chapters from KSP reports using font-size heuristics.\n",
    "\n",
    "    Approach:\n",
    "    - Scan all text spans, collect font sizes\n",
    "    - Use the largest font sizes to identify chapter/sub-chapter headers\n",
    "    - Group content between headers into chapter entries\n",
    "    - Return sub-chapter level entries with full text content\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pdf_path: str):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.filename = Path(pdf_path).stem\n",
    "        self.metadata = self._extract_metadata()\n",
    "\n",
    "    def _extract_metadata(self) -> Dict:\n",
    "        \"\"\"Extract metadata from filename (YYYY_CCC_Title.pdf).\"\"\"\n",
    "        pattern = r'(\\d{4})_([A-Z]{3})_(.+)'\n",
    "        match = re.match(pattern, self.filename)\n",
    "        if match:\n",
    "            return {\n",
    "                'year': match.group(1),\n",
    "                'country': match.group(2),\n",
    "                'title': match.group(3).replace('_', ' '),\n",
    "                'filename': self.filename\n",
    "            }\n",
    "        return {'filename': self.filename}\n",
    "\n",
    "    def _collect_text_blocks(self) -> List[Dict]:\n",
    "        \"\"\"Collect all text blocks with font size and page info.\"\"\"\n",
    "        doc = fitz.open(self.pdf_path)\n",
    "        blocks = []\n",
    "\n",
    "        for page_num, page in enumerate(doc, 1):\n",
    "            page_dict = page.get_text('dict')\n",
    "            for block in page_dict['blocks']:\n",
    "                if 'lines' not in block:\n",
    "                    continue\n",
    "                for line in block['lines']:\n",
    "                    text = ''\n",
    "                    max_font_size = 0\n",
    "                    is_bold = False\n",
    "                    for span in line['spans']:\n",
    "                        text += span['text']\n",
    "                        max_font_size = max(max_font_size, span['size'])\n",
    "                        if 'bold' in span.get('font', '').lower() or 'Bold' in span.get('font', ''):\n",
    "                            is_bold = True\n",
    "                    text = text.strip()\n",
    "                    if text:\n",
    "                        blocks.append({\n",
    "                            'text': text,\n",
    "                            'font_size': round(max_font_size, 1),\n",
    "                            'is_bold': is_bold,\n",
    "                            'page': page_num\n",
    "                        })\n",
    "\n",
    "        doc.close()\n",
    "        return blocks\n",
    "\n",
    "    def _identify_header_thresholds(self, blocks: List[Dict]) -> Tuple[float, float]:\n",
    "        \"\"\"Identify font-size thresholds for chapter vs sub-chapter headers.\n",
    "\n",
    "        Returns (chapter_threshold, subchapter_threshold).\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        size_counts = Counter()\n",
    "        for b in blocks:\n",
    "            size_counts[b['font_size']] += 1\n",
    "\n",
    "        # Sort sizes descending\n",
    "        sizes = sorted(size_counts.keys(), reverse=True)\n",
    "\n",
    "        # Body text is typically the most frequent font size\n",
    "        body_size = size_counts.most_common(1)[0][0]\n",
    "\n",
    "        # Sizes larger than body text are potential headers\n",
    "        header_sizes = [s for s in sizes if s > body_size]\n",
    "\n",
    "        if len(header_sizes) >= 2:\n",
    "            # Largest = chapter, second largest = sub-chapter\n",
    "            chapter_threshold = header_sizes[0]\n",
    "            subchapter_threshold = header_sizes[1]\n",
    "        elif len(header_sizes) == 1:\n",
    "            # Only one header size: treat as chapter, use bold for sub-chapters\n",
    "            chapter_threshold = header_sizes[0]\n",
    "            subchapter_threshold = header_sizes[0]\n",
    "        else:\n",
    "            # No clear header sizes; use body + 2 as threshold\n",
    "            chapter_threshold = body_size + 4\n",
    "            subchapter_threshold = body_size + 2\n",
    "\n",
    "        return chapter_threshold, subchapter_threshold\n",
    "\n",
    "    def extract_chapters(self) -> List[Dict]:\n",
    "        \"\"\"Extract chapters/sub-chapters from the PDF.\n",
    "\n",
    "        Returns list of dicts with:\n",
    "        - chapter_title: str\n",
    "        - chapter_level: int (1=chapter, 2=sub-chapter)\n",
    "        - content: str (full text of the chapter/sub-chapter)\n",
    "        - page_start: int\n",
    "        - page_end: int\n",
    "        - content_length: int\n",
    "        \"\"\"\n",
    "        blocks = self._collect_text_blocks()\n",
    "        if not blocks:\n",
    "            return []\n",
    "\n",
    "        ch_thresh, sub_thresh = self._identify_header_thresholds(blocks)\n",
    "\n",
    "        # Build chapters by scanning blocks\n",
    "        chapters = []\n",
    "        current = {\n",
    "            'chapter_title': 'Preamble',\n",
    "            'chapter_level': 0,\n",
    "            'content_parts': [],\n",
    "            'page_start': 1,\n",
    "            'page_end': 1\n",
    "        }\n",
    "\n",
    "        for block in blocks:\n",
    "            is_chapter_header = (\n",
    "                block['font_size'] >= ch_thresh\n",
    "                and len(block['text']) < 200\n",
    "                and len(block['text']) > 2\n",
    "            )\n",
    "            is_subchapter_header = (\n",
    "                not is_chapter_header\n",
    "                and block['font_size'] >= sub_thresh\n",
    "                and block['font_size'] > (sub_thresh - 0.5)\n",
    "                and len(block['text']) < 200\n",
    "                and len(block['text']) > 2\n",
    "                and (block['is_bold'] or block['font_size'] > sub_thresh)\n",
    "            )\n",
    "\n",
    "            if is_chapter_header or is_subchapter_header:\n",
    "                # Save previous chapter if it has content\n",
    "                content = '\\n'.join(current['content_parts']).strip()\n",
    "                if content and len(content) > 50:\n",
    "                    chapters.append({\n",
    "                        'chapter_title': current['chapter_title'],\n",
    "                        'chapter_level': current['chapter_level'],\n",
    "                        'content': content,\n",
    "                        'page_start': current['page_start'],\n",
    "                        'page_end': current['page_end'],\n",
    "                        'content_length': len(content)\n",
    "                    })\n",
    "\n",
    "                # Start new chapter\n",
    "                level = 1 if is_chapter_header else 2\n",
    "                current = {\n",
    "                    'chapter_title': block['text'],\n",
    "                    'chapter_level': level,\n",
    "                    'content_parts': [],\n",
    "                    'page_start': block['page'],\n",
    "                    'page_end': block['page']\n",
    "                }\n",
    "            else:\n",
    "                current['content_parts'].append(block['text'])\n",
    "                current['page_end'] = block['page']\n",
    "\n",
    "        # Add final chapter\n",
    "        content = '\\n'.join(current['content_parts']).strip()\n",
    "        if content and len(content) > 50:\n",
    "            chapters.append({\n",
    "                'chapter_title': current['chapter_title'],\n",
    "                'chapter_level': current['chapter_level'],\n",
    "                'content': content,\n",
    "                'page_start': current['page_start'],\n",
    "                'page_end': current['page_end'],\n",
    "                'content_length': len(content)\n",
    "            })\n",
    "\n",
    "        return chapters\n",
    "\n",
    "    def get_summary(self) -> Dict:\n",
    "        \"\"\"Get a summary of the extracted chapters.\"\"\"\n",
    "        chapters = self.extract_chapters()\n",
    "        return {\n",
    "            'filename': self.filename,\n",
    "            'metadata': self.metadata,\n",
    "            'num_chapters': len(chapters),\n",
    "            'chapters': [\n",
    "                {\n",
    "                    'title': ch['chapter_title'],\n",
    "                    'level': ch['chapter_level'],\n",
    "                    'pages': f\"{ch['page_start']}-{ch['page_end']}\",\n",
    "                    'chars': ch['content_length']\n",
    "                }\n",
    "                for ch in chapters\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "print('\\u2713 ChapterExtractor class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chapter extraction on one report\n",
    "ksp_pdfs = list(Path(config.ksp_dir).glob('*.pdf'))\n",
    "if ksp_pdfs:\n",
    "    test_pdf = str(ksp_pdfs[0])\n",
    "    print(f'Testing on: {Path(test_pdf).name}')\n",
    "    print()\n",
    "\n",
    "    extractor = ChapterExtractor(test_pdf)\n",
    "    print(f'Metadata: {extractor.metadata}')\n",
    "\n",
    "    chapters = extractor.extract_chapters()\n",
    "    print(f'\\nExtracted {len(chapters)} chapters/sub-chapters')\n",
    "    print()\n",
    "    for i, ch in enumerate(chapters[:15]):\n",
    "        level_marker = '  ' * ch['chapter_level']\n",
    "        print(f\"{level_marker}[L{ch['chapter_level']}] {ch['chapter_title'][:80]}\")\n",
    "        print(f\"{level_marker}     Pages {ch['page_start']}-{ch['page_end']}, {ch['content_length']} chars\")\n",
    "    if len(chapters) > 15:\n",
    "        print(f'  ... and {len(chapters) - 15} more')\n",
    "else:\n",
    "    print(f'No KSP reports found in: {config.ksp_dir}')\n",
    "    print('Please upload PDF reports to this directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Textbook Vector Store (for Theory Linking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manage embeddings and ChromaDB vector database for textbooks.\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str, persist_directory: str = None):\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        # Initialize embedding model\n",
    "        print(f'Loading embedding model: {config.embedding_model}...')\n",
    "        self.embedding_model = SentenceTransformer(config.embedding_model)\n",
    "        print('Embedding model loaded')\n",
    "\n",
    "        # Initialize ChromaDB with persistence\n",
    "        if persist_directory is None:\n",
    "            persist_directory = config.vector_db_dir\n",
    "\n",
    "        self.client = chromadb.Client(Settings(\n",
    "            persist_directory=persist_directory,\n",
    "            anonymized_telemetry=False\n",
    "        ))\n",
    "\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(name=collection_name)\n",
    "            print(f'Loaded existing collection: {collection_name}')\n",
    "        except:\n",
    "            self.collection = self.client.create_collection(\n",
    "                name=collection_name,\n",
    "                metadata={'description': f'Collection for {collection_name}'}\n",
    "            )\n",
    "            print(f'Created new collection: {collection_name}')\n",
    "\n",
    "    def add_documents(self, chunks: List[Dict], batch_size: int = 32):\n",
    "        \"\"\"Add document chunks to vector database.\"\"\"\n",
    "        documents = [chunk['text'] for chunk in chunks]\n",
    "        metadatas = [chunk['metadata'] for chunk in chunks]\n",
    "        ids = [chunk['metadata']['chunk_id'] for chunk in chunks]\n",
    "\n",
    "        print(f'Adding {len(documents)} documents to {self.collection_name}...')\n",
    "\n",
    "        all_embeddings = []\n",
    "        for i in tqdm(range(0, len(documents), batch_size), desc='Embedding'):\n",
    "            batch_docs = documents[i:i+batch_size]\n",
    "            embeddings = self.embedding_model.encode(\n",
    "                batch_docs, show_progress_bar=False, convert_to_numpy=True\n",
    "            ).tolist()\n",
    "            all_embeddings.extend(embeddings)\n",
    "\n",
    "        for i in tqdm(range(0, len(documents), batch_size), desc='Storing'):\n",
    "            batch_end = min(i + batch_size, len(documents))\n",
    "            self.collection.add(\n",
    "                documents=documents[i:batch_end],\n",
    "                embeddings=all_embeddings[i:batch_end],\n",
    "                metadatas=metadatas[i:batch_end],\n",
    "                ids=ids[i:batch_end]\n",
    "            )\n",
    "\n",
    "        print(f'\\u2713 Added {len(documents)} chunks to collection')\n",
    "\n",
    "    def search(self, query: str, n_results: int = 5, filter_dict: Dict = None) -> Dict:\n",
    "        \"\"\"Search for relevant chunks.\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])[0].tolist()\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=n_results,\n",
    "            where=filter_dict\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Get collection statistics.\"\"\"\n",
    "        return {\n",
    "            'collection_name': self.collection_name,\n",
    "            'total_chunks': self.collection.count()\n",
    "        }\n",
    "\n",
    "\n",
    "print('\\u2713 VectorStore class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize textbook vector store\n",
    "print('\\n' + '='*60)\n",
    "print('INITIALIZING TEXTBOOK VECTOR DATABASE')\n",
    "print('='*60)\n",
    "\n",
    "textbook_store = VectorStore(\n",
    "    collection_name=config.textbook_collection,\n",
    "    persist_directory=config.vector_db_dir\n",
    ")\n",
    "\n",
    "print(f'\\n\\u2713 Textbook store initialized')\n",
    "print(f'  ChromaDB persisted to: {config.vector_db_dir}')\n",
    "print(f'  Current stats: {textbook_store.get_stats()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index textbooks (run once; skip if already indexed)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def process_and_index_textbooks():\n",
    "    \"\"\"Process textbooks and add to vector database.\"\"\"\n",
    "    textbook_dir = Path(config.textbook_dir)\n",
    "    pdf_files = list(textbook_dir.glob('*.pdf'))\n",
    "\n",
    "    print(f'\\nFound {len(pdf_files)} textbooks')\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config.textbook_chunk_size,\n",
    "        chunk_overlap=config.chunk_overlap,\n",
    "        separators=['\\n\\n', '\\n', '. ', ' ', ''],\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        print(f'\\nProcessing: {pdf_path.name}')\n",
    "\n",
    "        # Extract text with sections via PyMuPDF\n",
    "        doc = fitz.open(str(pdf_path))\n",
    "        sections = []\n",
    "        current_section = {'title': 'Introduction', 'content': '', 'page': 1}\n",
    "\n",
    "        for page_num, page in enumerate(doc, 1):\n",
    "            blocks = page.get_text('dict')['blocks']\n",
    "            for block in blocks:\n",
    "                if 'lines' not in block:\n",
    "                    continue\n",
    "                for line in block['lines']:\n",
    "                    text = ''\n",
    "                    font_size = 0\n",
    "                    for span in line['spans']:\n",
    "                        text += span['text']\n",
    "                        font_size = max(font_size, span['size'])\n",
    "                    text = text.strip()\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    if font_size > 12 and len(text) < 100:\n",
    "                        if current_section['content'].strip():\n",
    "                            sections.append(current_section)\n",
    "                        current_section = {'title': text, 'content': '', 'page': page_num}\n",
    "                    else:\n",
    "                        current_section['content'] += text + '\\n'\n",
    "\n",
    "        if current_section['content'].strip():\n",
    "            sections.append(current_section)\n",
    "        doc.close()\n",
    "\n",
    "        print(f'  Extracted {len(sections)} sections')\n",
    "\n",
    "        # Chunk sections\n",
    "        filename = pdf_path.stem\n",
    "        metadata_base = {'source_type': 'textbook', 'filename': filename}\n",
    "\n",
    "        for s_idx, section in enumerate(sections):\n",
    "            sub_chunks = splitter.split_text(section['content'])\n",
    "            for c_idx, chunk_text in enumerate(sub_chunks):\n",
    "                all_chunks.append({\n",
    "                    'text': chunk_text,\n",
    "                    'metadata': {\n",
    "                        **metadata_base,\n",
    "                        'section_title': section['title'],\n",
    "                        'section_index': s_idx,\n",
    "                        'section_page': section['page'],\n",
    "                        'chunk_index': c_idx,\n",
    "                        'chunk_id': f'{filename}_s{s_idx}_c{c_idx}'\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        print(f'  Created {sum(1 for c in all_chunks if c[\"metadata\"][\"filename\"] == filename)} chunks')\n",
    "\n",
    "    # Add to vector database\n",
    "    if all_chunks:\n",
    "        textbook_store.add_documents(all_chunks)\n",
    "\n",
    "        output_path = f'{config.processed_dir}/textbook_chunks.json'\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(all_chunks, f, indent=2)\n",
    "        print(f'\\n\\u2713 Saved processed chunks to: {output_path}')\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# Check if textbooks are already indexed\n",
    "stats = textbook_store.get_stats()\n",
    "if stats['total_chunks'] > 0:\n",
    "    print(f'\\n\\u2713 Textbooks already indexed: {stats[\"total_chunks\"]} chunks')\n",
    "    print('  Skipping re-indexing. Delete collection to re-index.')\n",
    "else:\n",
    "    textbook_pdfs = list(Path(config.textbook_dir).glob('*.pdf'))\n",
    "    if len(textbook_pdfs) == 0:\n",
    "        print(f'\\n\\u26a0 No textbooks found in: {config.textbook_dir}')\n",
    "        print('Please upload textbook PDFs to this directory')\n",
    "    else:\n",
    "        print(f'\\n\\u2713 Found {len(textbook_pdfs)} textbooks. Indexing...')\n",
    "        textbook_chunks = process_and_index_textbooks()\n",
    "        print(f'\\nTextbook Collection: {textbook_store.get_stats()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Process All KSP Reports (Chapter Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chapters() -> Dict[str, List[Dict]]:\n",
    "    \"\"\"Extract chapters from all KSP PDF reports.\n",
    "\n",
    "    Returns dict mapping report_id -> list of chapter dicts.\n",
    "    \"\"\"\n",
    "    ksp_dir = Path(config.ksp_dir)\n",
    "    pdf_files = sorted(ksp_dir.glob('*.pdf'))\n",
    "\n",
    "    print(f'Found {len(pdf_files)} KSP reports')\n",
    "\n",
    "    all_reports = {}\n",
    "\n",
    "    for pdf_path in pdf_files:\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'Processing: {pdf_path.name}')\n",
    "        print('='*60)\n",
    "\n",
    "        extractor = ChapterExtractor(str(pdf_path))\n",
    "        chapters = extractor.extract_chapters()\n",
    "\n",
    "        report_id = extractor.filename\n",
    "        all_reports[report_id] = {\n",
    "            'metadata': extractor.metadata,\n",
    "            'chapters': chapters\n",
    "        }\n",
    "\n",
    "        print(f'  Extracted {len(chapters)} chapters/sub-chapters')\n",
    "        total_chars = sum(ch['content_length'] for ch in chapters)\n",
    "        print(f'  Total text: {total_chars:,} characters')\n",
    "\n",
    "        # Show chapter outline\n",
    "        for ch in chapters[:10]:\n",
    "            indent = '  ' * ch['chapter_level']\n",
    "            print(f\"  {indent}[L{ch['chapter_level']}] {ch['chapter_title'][:70]}  ({ch['content_length']} chars)\")\n",
    "        if len(chapters) > 10:\n",
    "            print(f'  ... and {len(chapters) - 10} more')\n",
    "\n",
    "    # Save chapter summaries\n",
    "    summary = {}\n",
    "    for report_id, data in all_reports.items():\n",
    "        summary[report_id] = {\n",
    "            'metadata': data['metadata'],\n",
    "            'num_chapters': len(data['chapters']),\n",
    "            'chapters': [\n",
    "                {\n",
    "                    'title': ch['chapter_title'],\n",
    "                    'level': ch['chapter_level'],\n",
    "                    'pages': f\"{ch['page_start']}-{ch['page_end']}\",\n",
    "                    'chars': ch['content_length']\n",
    "                }\n",
    "                for ch in data['chapters']\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    summary_path = f'{config.processed_dir}/chapter_summaries.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f'\\n\\u2713 Chapter summaries saved to: {summary_path}')\n",
    "\n",
    "    return all_reports\n",
    "\n",
    "\n",
    "# Extract chapters from all reports\n",
    "print('\\n' + '='*60)\n",
    "print('PHASE 1: CHAPTER EXTRACTION')\n",
    "print('='*60)\n",
    "\n",
    "all_reports = extract_all_chapters()\n",
    "\n",
    "total_chapters = sum(len(data['chapters']) for data in all_reports.values())\n",
    "print(f'\\n\\u2713 Total: {total_chapters} chapters/sub-chapters from {len(all_reports)} reports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: LLM Classification + Extraction (Combined Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import time\n",
    "\n",
    "\n",
    "class ChapterAnalyzer:\n",
    "    \"\"\"Analyze KSP report chapters using a combined LLM prompt.\n",
    "\n",
    "    For each chapter, performs:\n",
    "    1. Taxonomy sector/keyword classification\n",
    "    2. Knowledge type classification\n",
    "    3. Korean policy extraction\n",
    "    4. Theory linking (using textbook RAG context)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, textbook_store: VectorStore):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.model = config.llm_model\n",
    "        self.textbook_store = textbook_store\n",
    "        self.taxonomy_string = taxonomy_to_prompt_string()\n",
    "\n",
    "    def _get_theory_context(self, chapter_title: str, chapter_content: str) -> str:\n",
    "        \"\"\"Query textbook store for relevant theory passages.\"\"\"\n",
    "        # Build a concise query from the chapter topic\n",
    "        query = f'{chapter_title} development policy economic theory'\n",
    "        # Use first 200 chars of content for better query\n",
    "        content_snippet = chapter_content[:200].replace('\\n', ' ')\n",
    "        query = f'{chapter_title} {content_snippet}'\n",
    "\n",
    "        try:\n",
    "            results = self.textbook_store.search(\n",
    "                query=query,\n",
    "                n_results=config.textbook_top_k\n",
    "            )\n",
    "            if results['documents'][0]:\n",
    "                return '\\n\\n---\\n\\n'.join(results['documents'][0])\n",
    "        except Exception as e:\n",
    "            print(f'  Warning: Theory retrieval failed: {e}')\n",
    "\n",
    "        return ''\n",
    "\n",
    "    def _build_prompt(self, chapter_title: str, chapter_content: str,\n",
    "                      theory_context: str) -> str:\n",
    "        \"\"\"Build the combined classification + extraction prompt.\"\"\"\n",
    "\n",
    "        # Truncate very long chapters to fit in context\n",
    "        max_content_chars = 15000\n",
    "        if len(chapter_content) > max_content_chars:\n",
    "            chapter_content = chapter_content[:max_content_chars] + '\\n\\n[... content truncated ...]'\n",
    "\n",
    "        theory_section = theory_context if theory_context else 'No textbook passages retrieved.'\n",
    "\n",
    "        return f\"\"\"You are analyzing a chapter from a KSP (Knowledge Sharing Program) development cooperation report.\n",
    "\n",
    "CHAPTER TITLE: {chapter_title}\n",
    "\n",
    "CHAPTER CONTENT:\n",
    "{chapter_content}\n",
    "\n",
    "RELATED THEORETICAL PASSAGES (from development economics textbooks):\n",
    "{theory_section}\n",
    "\n",
    "DEVELOPMENT COOPERATION TAXONOMY:\n",
    "{self.taxonomy_string}\n",
    "\n",
    "KNOWLEDGE TYPES:\n",
    "1. Contextual background and situation analysis\n",
    "2. Policy implementation and coordinating mechanism\n",
    "3. Technical methodology and analytical framework\n",
    "4. Recommendations and future directions\n",
    "\n",
    "TASK: Perform ALL of the following analyses on this chapter.\n",
    "\n",
    "1. TAXONOMY CLASSIFICATION: Identify which sector(s) and keywords from the taxonomy above best describe this chapter's content. A chapter may map to multiple sectors. Select the most specific keywords that apply.\n",
    "\n",
    "2. KNOWLEDGE TYPE: Classify this chapter as one of the 4 knowledge types listed above.\n",
    "\n",
    "3. KOREAN POLICY EXTRACTION: Extract any Korean government policies, programs, or initiatives described in this chapter. For each policy provide: policy_name, year_initiated (null if not stated), organization (responsible ministry/agency, null if not stated), challenge_addressed, policy_instruments (list of specific mechanisms), sector. Each policy MUST include an evidence_quote (verbatim from the chapter). If this chapter does NOT contain any Korean policy experiences, return \"Not Applicable\" for this field.\n",
    "\n",
    "4. THEORY LINKING: Based on the textbook passages provided, identify any related theoretical concepts or frameworks. If no relevant theory link exists, return \"Not Applicable\".\n",
    "\n",
    "OUTPUT FORMAT: Return ONLY valid JSON (no markdown, no preamble). Use this exact structure:\n",
    "\n",
    "{{\n",
    "  \"taxonomy_classification\": {{\n",
    "    \"sectors\": [\n",
    "      {{\n",
    "        \"sector\": \"(N) Sector Name\",\n",
    "        \"sub_sector_l1\": \"Sub-sector Level 1 name\",\n",
    "        \"sub_sector_l2\": \"Sub-sector Level 2 name\",\n",
    "        \"keywords\": [\"Keyword 1\", \"Keyword 2\"]\n",
    "      }}\n",
    "    ],\n",
    "    \"knowledge_type\": \"one of the 4 knowledge types\",\n",
    "    \"confidence\": \"high\" | \"medium\" | \"low\",\n",
    "    \"reasoning\": \"Brief explanation of classification\"\n",
    "  }},\n",
    "  \"korean_policies\": [\n",
    "    {{\n",
    "      \"policy_name\": \"string\",\n",
    "      \"year_initiated\": integer or null,\n",
    "      \"organization\": \"string\" or null,\n",
    "      \"challenge_addressed\": \"string\",\n",
    "      \"policy_instruments\": [\"string\"],\n",
    "      \"sector\": \"string\",\n",
    "      \"evidence_quote\": \"verbatim quote from chapter\"\n",
    "    }}\n",
    "  ],\n",
    "  \"related_theories\": [\n",
    "    {{\n",
    "      \"theory\": \"Theory name and source\",\n",
    "      \"relevance\": \"How this theory relates to the chapter content\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "CRITICAL RULES:\n",
    "- Use ONLY keywords that exist in the taxonomy above\n",
    "- If no Korean policies are found, set \"korean_policies\" to \"Not Applicable\"\n",
    "- If no theory links are found, set \"related_theories\" to \"Not Applicable\"\n",
    "- taxonomy_classification and knowledge_type are ALWAYS required\n",
    "- evidence_quote must be verbatim from the chapter content\n",
    "- Return valid JSON only\"\"\"\n",
    "\n",
    "    def analyze_chapter(self, chapter: Dict, report_id: str) -> Dict:\n",
    "        \"\"\"Analyze a single chapter with the combined prompt.\"\"\"\n",
    "\n",
    "        chapter_title = chapter['chapter_title']\n",
    "        chapter_content = chapter['content']\n",
    "\n",
    "        # Get theory context from textbook store\n",
    "        theory_context = self._get_theory_context(chapter_title, chapter_content)\n",
    "\n",
    "        # Build and send prompt\n",
    "        prompt = self._build_prompt(chapter_title, chapter_content, theory_context)\n",
    "\n",
    "        try:\n",
    "            response = self.client.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=config.max_tokens,\n",
    "                temperature=config.temperature,\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "\n",
    "            content = response.content[0].text.strip()\n",
    "\n",
    "            # Clean response\n",
    "            if content.startswith('```json'):\n",
    "                content = content[7:]\n",
    "            if content.startswith('```'):\n",
    "                content = content[3:]\n",
    "            if content.endswith('```'):\n",
    "                content = content[:-3]\n",
    "            content = content.strip()\n",
    "\n",
    "            result = json.loads(content)\n",
    "\n",
    "            # Attach chapter metadata\n",
    "            return {\n",
    "                'report_id': report_id,\n",
    "                'chapter_title': chapter_title,\n",
    "                'chapter_level': chapter['chapter_level'],\n",
    "                'page_start': chapter['page_start'],\n",
    "                'page_end': chapter['page_end'],\n",
    "                'content_length': chapter['content_length'],\n",
    "                **result\n",
    "            }\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f'  JSON parse error for \"{chapter_title}\": {e}')\n",
    "            return {\n",
    "                'report_id': report_id,\n",
    "                'chapter_title': chapter_title,\n",
    "                'chapter_level': chapter['chapter_level'],\n",
    "                'page_start': chapter['page_start'],\n",
    "                'page_end': chapter['page_end'],\n",
    "                'content_length': chapter['content_length'],\n",
    "                'error': f'JSON parse error: {str(e)}',\n",
    "                'raw_response': content[:500]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f'  API error for \"{chapter_title}\": {e}')\n",
    "            return {\n",
    "                'report_id': report_id,\n",
    "                'chapter_title': chapter_title,\n",
    "                'chapter_level': chapter['chapter_level'],\n",
    "                'page_start': chapter['page_start'],\n",
    "                'page_end': chapter['page_end'],\n",
    "                'content_length': chapter['content_length'],\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = ChapterAnalyzer(\n",
    "    api_key=ANTHROPIC_API_KEY,\n",
    "    textbook_store=textbook_store\n",
    ")\n",
    "print('\\u2713 ChapterAnalyzer initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Run Analysis on All Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_analysis(all_reports: Dict, analyzer: ChapterAnalyzer) -> List[Dict]:\n",
    "    \"\"\"Run combined classification + extraction on all chapters of all reports.\"\"\"\n",
    "\n",
    "    all_results = []\n",
    "    total_chapters = sum(len(data['chapters']) for data in all_reports.values())\n",
    "    processed = 0\n",
    "\n",
    "    for report_id, data in all_reports.items():\n",
    "        chapters = data['chapters']\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'ANALYZING: {report_id}')\n",
    "        print(f'  {len(chapters)} chapters to process')\n",
    "        print('='*60)\n",
    "\n",
    "        for i, chapter in enumerate(chapters):\n",
    "            processed += 1\n",
    "            print(f'\\n  [{processed}/{total_chapters}] \"{chapter[\"chapter_title\"][:60]}...\"')\n",
    "            print(f'    Pages {chapter[\"page_start\"]}-{chapter[\"page_end\"]}, {chapter[\"content_length\"]} chars')\n",
    "\n",
    "            result = analyzer.analyze_chapter(chapter, report_id)\n",
    "            all_results.append(result)\n",
    "\n",
    "            # Show brief result\n",
    "            if 'taxonomy_classification' in result:\n",
    "                tc = result['taxonomy_classification']\n",
    "                sectors = [s['sector'] for s in tc.get('sectors', [])]\n",
    "                kt = tc.get('knowledge_type', 'N/A')\n",
    "                print(f'    Sectors: {sectors}')\n",
    "                print(f'    Knowledge type: {kt}')\n",
    "\n",
    "                policies = result.get('korean_policies', 'Not Applicable')\n",
    "                if isinstance(policies, list):\n",
    "                    print(f'    Korean policies: {len(policies)} found')\n",
    "                else:\n",
    "                    print(f'    Korean policies: {policies}')\n",
    "            elif 'error' in result:\n",
    "                print(f'    ERROR: {result[\"error\"][:80]}')\n",
    "\n",
    "            # Rate limiting pause\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Save results\n",
    "    output_path = f'{config.results_dir}/chapter_analysis.json'\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print('ANALYSIS COMPLETE')\n",
    "    print('='*60)\n",
    "    print(f'Total chapters analyzed: {len(all_results)}')\n",
    "    print(f'Results saved to: {output_path}')\n",
    "\n",
    "    # Summary stats\n",
    "    errors = sum(1 for r in all_results if 'error' in r)\n",
    "    with_policies = sum(\n",
    "        1 for r in all_results\n",
    "        if isinstance(r.get('korean_policies'), list) and len(r['korean_policies']) > 0\n",
    "    )\n",
    "    total_policies = sum(\n",
    "        len(r['korean_policies'])\n",
    "        for r in all_results\n",
    "        if isinstance(r.get('korean_policies'), list)\n",
    "    )\n",
    "\n",
    "    print(f'\\nErrors: {errors}')\n",
    "    print(f'Chapters with Korean policies: {with_policies}')\n",
    "    print(f'Total Korean policies extracted: {total_policies}')\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "print('\\n' + '='*60)\n",
    "print('PHASE 2: CHAPTER-LEVEL CLASSIFICATION & EXTRACTION')\n",
    "print('='*60)\n",
    "\n",
    "analysis_results = run_full_analysis(all_reports, analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def visualize_results(results: List[Dict]):\n",
    "    \"\"\"Generate visualizations from chapter analysis results.\"\"\"\n",
    "\n",
    "    # Filter out errors\n",
    "    valid = [r for r in results if 'taxonomy_classification' in r]\n",
    "    print(f'Visualizing {len(valid)} successfully analyzed chapters ({len(results) - len(valid)} errors)\\n')\n",
    "\n",
    "    if not valid:\n",
    "        print('No valid results to visualize')\n",
    "        return\n",
    "\n",
    "    # --- 1. Sector Distribution ---\n",
    "    sector_counts = Counter()\n",
    "    for r in valid:\n",
    "        for s in r['taxonomy_classification'].get('sectors', []):\n",
    "            sector_counts[s['sector']] += 1\n",
    "\n",
    "    if sector_counts:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sectors = sorted(sector_counts.keys())\n",
    "        counts = [sector_counts[s] for s in sectors]\n",
    "        # Wrap long sector names\n",
    "        labels = [s.replace('&', '&\\n') if len(s) > 25 else s for s in sectors]\n",
    "        ax.barh(labels, counts, color='steelblue')\n",
    "        ax.set_xlabel('Number of Chapters')\n",
    "        ax.set_title('Sector Distribution Across All Chapters')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{config.results_dir}/sector_distribution.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print('\\u2713 Saved sector_distribution.png')\n",
    "\n",
    "    # --- 2. Knowledge Type Distribution ---\n",
    "    kt_counts = Counter()\n",
    "    for r in valid:\n",
    "        kt = r['taxonomy_classification'].get('knowledge_type', 'Unknown')\n",
    "        kt_counts[kt] += 1\n",
    "\n",
    "    if kt_counts:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        kt_labels = list(kt_counts.keys())\n",
    "        kt_vals = [kt_counts[k] for k in kt_labels]\n",
    "        # Wrap long labels\n",
    "        wrapped_labels = []\n",
    "        for label in kt_labels:\n",
    "            if len(label) > 35:\n",
    "                words = label.split()\n",
    "                mid = len(words) // 2\n",
    "                wrapped_labels.append(' '.join(words[:mid]) + '\\n' + ' '.join(words[mid:]))\n",
    "            else:\n",
    "                wrapped_labels.append(label)\n",
    "        ax.barh(wrapped_labels, kt_vals, color='coral')\n",
    "        ax.set_xlabel('Number of Chapters')\n",
    "        ax.set_title('Knowledge Type Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{config.results_dir}/knowledge_type_distribution.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print('\\u2713 Saved knowledge_type_distribution.png')\n",
    "\n",
    "    # --- 3. Sector x Knowledge Type Heatmap ---\n",
    "    if sector_counts and kt_counts:\n",
    "        cross_data = defaultdict(lambda: defaultdict(int))\n",
    "        for r in valid:\n",
    "            kt = r['taxonomy_classification'].get('knowledge_type', 'Unknown')\n",
    "            for s in r['taxonomy_classification'].get('sectors', []):\n",
    "                cross_data[s['sector']][kt] += 1\n",
    "\n",
    "        df_cross = pd.DataFrame(cross_data).fillna(0).astype(int)\n",
    "        if not df_cross.empty:\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            sns.heatmap(df_cross, annot=True, fmt='d', cmap='YlOrRd', ax=ax)\n",
    "            ax.set_title('Sector x Knowledge Type Heatmap')\n",
    "            ax.set_ylabel('Knowledge Type')\n",
    "            ax.set_xlabel('Sector')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{config.results_dir}/sector_knowledge_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            print('\\u2713 Saved sector_knowledge_heatmap.png')\n",
    "\n",
    "    # --- 4. Per-Report Summary ---\n",
    "    print('\\n' + '='*60)\n",
    "    print('PER-REPORT SUMMARY')\n",
    "    print('='*60)\n",
    "\n",
    "    report_groups = defaultdict(list)\n",
    "    for r in valid:\n",
    "        report_groups[r['report_id']].append(r)\n",
    "\n",
    "    for report_id, chapters in report_groups.items():\n",
    "        print(f'\\n--- {report_id} ---')\n",
    "        print(f'  Chapters analyzed: {len(chapters)}')\n",
    "\n",
    "        # Policies\n",
    "        total_policies = 0\n",
    "        for ch in chapters:\n",
    "            policies = ch.get('korean_policies', 'Not Applicable')\n",
    "            if isinstance(policies, list):\n",
    "                total_policies += len(policies)\n",
    "        print(f'  Korean policies extracted: {total_policies}')\n",
    "\n",
    "        # Top sectors\n",
    "        ch_sectors = Counter()\n",
    "        for ch in chapters:\n",
    "            for s in ch['taxonomy_classification'].get('sectors', []):\n",
    "                ch_sectors[s['sector']] += 1\n",
    "        if ch_sectors:\n",
    "            top = ch_sectors.most_common(3)\n",
    "            print(f'  Top sectors: {[\", \".join(f\"{s}({c})\" for s, c in top)]}')\n",
    "\n",
    "        # Knowledge types\n",
    "        ch_kt = Counter(ch['taxonomy_classification'].get('knowledge_type', 'Unknown') for ch in chapters)\n",
    "        print(f'  Knowledge types: {dict(ch_kt)}')\n",
    "\n",
    "        # Show policies\n",
    "        if total_policies > 0:\n",
    "            print(f'  Policies:')\n",
    "            for ch in chapters:\n",
    "                policies = ch.get('korean_policies', 'Not Applicable')\n",
    "                if isinstance(policies, list):\n",
    "                    for p in policies:\n",
    "                        yr = p.get('year_initiated', '?')\n",
    "                        print(f'    - {p[\"policy_name\"]} ({yr})')\n",
    "\n",
    "\n",
    "# Run visualization\n",
    "if 'analysis_results' in locals() and analysis_results:\n",
    "    visualize_results(analysis_results)\n",
    "else:\n",
    "    # Try loading from file\n",
    "    results_path = f'{config.results_dir}/chapter_analysis.json'\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path) as f:\n",
    "            analysis_results = json.load(f)\n",
    "        print(f'Loaded {len(analysis_results)} results from file')\n",
    "        visualize_results(analysis_results)\n",
    "    else:\n",
    "        print('No analysis results found. Run Section 8 first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed chapter-level view for one report\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def show_detailed_report(results: List[Dict], report_id: str = None):\n",
    "    \"\"\"Show detailed chapter-by-chapter view for a specific report.\"\"\"\n",
    "\n",
    "    valid = [r for r in results if 'taxonomy_classification' in r]\n",
    "    if report_id:\n",
    "        report_chapters = [r for r in valid if r['report_id'] == report_id]\n",
    "    else:\n",
    "        # Show first report\n",
    "        if not valid:\n",
    "            print('No valid results')\n",
    "            return\n",
    "        report_id = valid[0]['report_id']\n",
    "        report_chapters = [r for r in valid if r['report_id'] == report_id]\n",
    "\n",
    "    print(f'\\nDETAILED VIEW: {report_id}')\n",
    "    print('='*80)\n",
    "\n",
    "    for ch in report_chapters:\n",
    "        indent = '  ' * ch.get('chapter_level', 0)\n",
    "        print(f'\\n{indent}[L{ch.get(\"chapter_level\", 0)}] {ch[\"chapter_title\"]}')\n",
    "        print(f'{indent}  Pages {ch[\"page_start\"]}-{ch[\"page_end\"]} | {ch[\"content_length\"]} chars')\n",
    "\n",
    "        tc = ch['taxonomy_classification']\n",
    "        for s in tc.get('sectors', []):\n",
    "            kws = ', '.join(s.get('keywords', []))\n",
    "            print(f'{indent}  Sector: {s[\"sector\"]} > {s.get(\"sub_sector_l1\", \"\")} > {s.get(\"sub_sector_l2\", \"\")}')\n",
    "            print(f'{indent}  Keywords: {kws}')\n",
    "        print(f'{indent}  Knowledge type: {tc.get(\"knowledge_type\", \"N/A\")}')\n",
    "        print(f'{indent}  Confidence: {tc.get(\"confidence\", \"N/A\")}')\n",
    "\n",
    "        policies = ch.get('korean_policies', 'Not Applicable')\n",
    "        if isinstance(policies, list) and policies:\n",
    "            for p in policies:\n",
    "                print(f'{indent}  Policy: {p[\"policy_name\"]} ({p.get(\"year_initiated\", \"?\")}) [{p.get(\"sector\", \"\")}]')\n",
    "                print(f'{indent}    Org: {p.get(\"organization\", \"N/A\")}')\n",
    "                print(f'{indent}    Challenge: {p.get(\"challenge_addressed\", \"N/A\")}')\n",
    "                instruments = p.get('policy_instruments', [])\n",
    "                if instruments:\n",
    "                    print(f'{indent}    Instruments: {\", \".join(instruments)}')\n",
    "                quote = p.get('evidence_quote', '')\n",
    "                if quote:\n",
    "                    print(f'{indent}    Evidence: \"{quote[:100]}...\"')\n",
    "        elif policies == 'Not Applicable':\n",
    "            print(f'{indent}  Korean policies: Not Applicable')\n",
    "\n",
    "        theories = ch.get('related_theories', 'Not Applicable')\n",
    "        if isinstance(theories, list) and theories:\n",
    "            for t in theories:\n",
    "                print(f'{indent}  Theory: {t.get(\"theory\", \"\")} - {t.get(\"relevance\", \"\")[:80]}')\n",
    "        elif theories == 'Not Applicable':\n",
    "            print(f'{indent}  Related theories: Not Applicable')\n",
    "\n",
    "    # Theory-Practice Network for this report\n",
    "    policies_for_network = []\n",
    "    for ch in report_chapters:\n",
    "        policies = ch.get('korean_policies', 'Not Applicable')\n",
    "        theories = ch.get('related_theories', 'Not Applicable')\n",
    "        if isinstance(policies, list):\n",
    "            for p in policies:\n",
    "                p_theories = theories if isinstance(theories, list) else []\n",
    "                policies_for_network.append((p, p_theories))\n",
    "\n",
    "    if policies_for_network:\n",
    "        G = nx.Graph()\n",
    "        for policy, theories in policies_for_network:\n",
    "            pname = policy['policy_name'][:35]\n",
    "            G.add_node(pname, node_type='policy')\n",
    "            for t in theories:\n",
    "                tname = t.get('theory', '')[:35]\n",
    "                if tname:\n",
    "                    G.add_node(tname, node_type='theory')\n",
    "                    G.add_edge(pname, tname)\n",
    "\n",
    "        if G.number_of_nodes() > 0:\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n",
    "            colors = ['lightblue' if G.nodes[n].get('node_type') == 'policy' else 'lightcoral' for n in G.nodes()]\n",
    "            nx.draw(G, pos, node_color=colors, with_labels=True, font_size=7,\n",
    "                    node_size=500, alpha=0.7, edge_color='gray')\n",
    "            plt.title(f'Theory-Practice Network: {report_id}\\n(Blue=Policies, Red=Theories)')\n",
    "            plt.tight_layout()\n",
    "            safe_name = report_id.replace(' ', '_')[:50]\n",
    "            plt.savefig(f'{config.results_dir}/network_{safe_name}.png', dpi=150, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Show detailed view for each report\n",
    "if 'analysis_results' in locals() and analysis_results:\n",
    "    report_ids = list(set(r['report_id'] for r in analysis_results if 'taxonomy_classification' in r))\n",
    "    for rid in report_ids:\n",
    "        show_detailed_report(analysis_results, rid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('CHAPTER-LEVEL ANALYSIS COMPLETE')\n",
    "print('='*60)\n",
    "\n",
    "print(f'\\nResults saved in: {config.project_dir}')\n",
    "print('\\nKey outputs:')\n",
    "print(f'  1. Chapter summaries: {config.processed_dir}/chapter_summaries.json')\n",
    "print(f'  2. Full analysis: {config.results_dir}/chapter_analysis.json')\n",
    "print(f'  3. Visualizations: {config.results_dir}/*.png')\n",
    "print(f'  4. Textbook vectors: {config.vector_db_dir}/')\n",
    "\n",
    "# Load and display summary stats\n",
    "if 'analysis_results' in locals() and analysis_results:\n",
    "    valid = [r for r in analysis_results if 'taxonomy_classification' in r]\n",
    "    errors = len(analysis_results) - len(valid)\n",
    "\n",
    "    total_policies = sum(\n",
    "        len(r['korean_policies'])\n",
    "        for r in valid\n",
    "        if isinstance(r.get('korean_policies'), list)\n",
    "    )\n",
    "\n",
    "    chapters_with_policies = sum(\n",
    "        1 for r in valid\n",
    "        if isinstance(r.get('korean_policies'), list) and len(r['korean_policies']) > 0\n",
    "    )\n",
    "\n",
    "    chapters_with_theories = sum(\n",
    "        1 for r in valid\n",
    "        if isinstance(r.get('related_theories'), list) and len(r['related_theories']) > 0\n",
    "    )\n",
    "\n",
    "    print(f'\\nSummary Statistics:')\n",
    "    print(f'  Reports processed: {len(set(r[\"report_id\"] for r in analysis_results))}')\n",
    "    print(f'  Chapters analyzed: {len(valid)} ({errors} errors)')\n",
    "    print(f'  Korean policies extracted: {total_policies} (in {chapters_with_policies} chapters)')\n",
    "    print(f'  Chapters with theory links: {chapters_with_theories}')\n",
    "\n",
    "    # Sector coverage\n",
    "    all_sectors = Counter()\n",
    "    for r in valid:\n",
    "        for s in r['taxonomy_classification'].get('sectors', []):\n",
    "            all_sectors[s['sector']] += 1\n",
    "    print(f'  Sector coverage: {len(all_sectors)} sectors used')\n",
    "    for sector, count in all_sectors.most_common():\n",
    "        print(f'    {sector}: {count} chapters')\n",
    "\n",
    "print('\\nNext steps:')\n",
    "print('  1. Review chapter_analysis.json for quality')\n",
    "print('  2. Check taxonomy classification accuracy')\n",
    "print('  3. Verify Korean policy extraction completeness')\n",
    "print('  4. Scale to full 566 KSP reports if quality is satisfactory')\n",
    "\n",
    "print('\\n' + '='*60)"
   ]
  }
 ]
}