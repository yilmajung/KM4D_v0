{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yilmajung/KM4D_v0/blob/main/ksp_nlm_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KSP Knowledge Extraction: NotebookLM vs Pipeline Comparison\n",
    "\n",
    "Compare chapter-level classifications from Google NotebookLM (manual, top-level chapters)\n",
    "against our automated pipeline (3 LLMs, sub-chapter level).\n",
    "\n",
    "| Source | Granularity | Chapters | Format |\n",
    "|--------|------------|----------|--------|\n",
    "| NotebookLM | Top-level chapters | 19 | CSV (manual) |\n",
    "| Claude Sonnet 4 | Sub-chapters | 112 (112 valid) | JSON (automated) |\n",
    "| Llama 3.1 8B | Sub-chapters | 112 (73 valid) | JSON (automated) |\n",
    "| Llama 3.1 70B | Sub-chapters | 112 (109 valid) | JSON (automated) |\n",
    "\n",
    "Two comparison approaches:\n",
    "1. **Chapter-level**: Aggregate pipeline sub-chapters to match NLM's top-level chapters\n",
    "2. **Report-level**: Compare distributions aggregated by report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data ---\n",
    "if os.path.exists('/content/drive/MyDrive/KM4D_v0/data/results'):\n",
    "    RESULTS_DIR = '/content/drive/MyDrive/KM4D_v0/data/results'\n",
    "else:\n",
    "    RESULTS_DIR = 'data/results'\n",
    "\n",
    "MODEL_FILES = {\n",
    "    'Claude Sonnet 4': 'chapter_analysis.json',\n",
    "    'Llama 3.1 8B': 'chapter_analysis_llama.json',\n",
    "    'Llama 3.1 70B': 'chapter_analysis_llama70b.json',\n",
    "}\n",
    "pipeline = {}\n",
    "for label, fname in MODEL_FILES.items():\n",
    "    with open(os.path.join(RESULTS_DIR, fname)) as f:\n",
    "        pipeline[label] = json.load(f)\n",
    "    print(f'{label}: {len(pipeline[label])} entries')\n",
    "\n",
    "MODEL_LABELS = list(MODEL_FILES.keys())\n",
    "\n",
    "nlm_path = os.path.join(RESULTS_DIR, 'results_nlm.csv')\n",
    "with open(nlm_path, encoding='cp1252') as f:\n",
    "    nlm_raw = list(csv.DictReader(f))\n",
    "print(f'NotebookLM: {len(nlm_raw)} CSV rows loaded')\n",
    "\n",
    "# Report ID mapping (NLM short IDs -> pipeline long IDs)\n",
    "pipeline_reports = sorted({d['report_id'] for d in pipeline[MODEL_LABELS[0]]})\n",
    "REPORT_MAP = {pid[:8]: pid for pid in pipeline_reports}\n",
    "\n",
    "ALL_LABELS = ['NotebookLM'] + MODEL_LABELS\n",
    "COLORS = {\n",
    "    'NotebookLM': '#ef4444',\n",
    "    'Claude Sonnet 4': '#6366f1',\n",
    "    'Llama 3.1 8B': '#f59e0b',\n",
    "    'Llama 3.1 70B': '#10b981',\n",
    "}\n",
    "\n",
    "# --- Pipeline sector normalization ---\n",
    "TAXONOMY_SECTORS = {\n",
    "    '(1) Economic Policy': [\n",
    "        'Macroeconomic Policy & Stability', 'Inclusive & Sustainable Growth',\n",
    "        'Investment & Private Sector Dev.'],\n",
    "    '(2) Social Services': [\n",
    "        'Education', 'Health', 'Social Protection & Inclusion',\n",
    "        'Cross-Cutting Social Issues'],\n",
    "    '(3) Digital Innovation': [\n",
    "        'Digital Policy & Governance', 'Digital Infrastructure',\n",
    "        'Digital Transformation', 'Emerging Technologies'],\n",
    "    '(4) Production & Trade': [\n",
    "        'Agriculture, Forestry & Fisheries', 'Industry & Services',\n",
    "        'Trade Policy & Facilitation'],\n",
    "    '(5) Infrastructure': [\n",
    "        'Infrastructure Policy & Finance', 'Transport',\n",
    "        'Water & Sanitation', 'Urban & Rural Development'],\n",
    "    '(6) Energy & Environment': [\n",
    "        'Environmental Policy & Management', 'Climate Change', 'Energy'],\n",
    "}\n",
    "SECTOR_NAMES = list(TAXONOMY_SECTORS.keys())\n",
    "\n",
    "_sector_lookup = {}\n",
    "for sector, l1_names in TAXONOMY_SECTORS.items():\n",
    "    short = sector.split(') ', 1)[1]\n",
    "    _sector_lookup[short.lower()] = sector\n",
    "    _sector_lookup[sector.lower()] = sector\n",
    "    for l1 in l1_names:\n",
    "        _sector_lookup[l1.lower()] = sector\n",
    "\n",
    "def normalize_sector(name):\n",
    "    \"\"\"Normalize any sector/sub-sector name to canonical form.\"\"\"\n",
    "    if not name: return 'Unknown'\n",
    "    if name in SECTOR_NAMES: return name\n",
    "    return _sector_lookup.get(name.lower(), f'Unknown ({name})')\n",
    "\n",
    "# --- NLM sector parsing ---\n",
    "NLM_SECTOR_KEYWORDS = [\n",
    "    ('Energy & Environment', '(6) Energy & Environment'),\n",
    "    ('Production & Trade', '(4) Production & Trade'),\n",
    "    ('Digital Innovation', '(3) Digital Innovation'),\n",
    "    ('Social Services', '(2) Social Services'),\n",
    "    ('Economic Policy', '(1) Economic Policy'),\n",
    "    ('Industry & Services', '(4) Production & Trade'),\n",
    "    ('Infrastructure', '(5) Infrastructure'),\n",
    "]\n",
    "\n",
    "def parse_nlm_sectors(raw):\n",
    "    \"\"\"Extract canonical sector names from NLM concatenated string.\"\"\"\n",
    "    found = []\n",
    "    for keyword, canonical in NLM_SECTOR_KEYWORDS:\n",
    "        idx = raw.find(keyword)\n",
    "        if idx >= 0:\n",
    "            found.append((idx, canonical))\n",
    "    found.sort()\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for _, canonical in found:\n",
    "        if canonical not in seen:\n",
    "            seen.add(canonical)\n",
    "            result.append(canonical)\n",
    "    return result if result else ['Unknown']\n",
    "\n",
    "# --- NLM knowledge type normalization ---\n",
    "KT_MAP = {\n",
    "    'Visionary Leadership': 'Policy Entrepreneurship (leadership)',\n",
    "    'Policy Entrepreneurship': 'Policy Entrepreneurship (leadership)',\n",
    "    'Policy Implementation & Coordinating Mechanism': 'Policy Implementation & Coordinating Mechanism',\n",
    "    'Policy Implementation': 'Policy Implementation & Coordinating Mechanism',\n",
    "    'Organizational / Individual Capacity': 'Organizational/Individual Capacity',\n",
    "    'Organizational Capacity': 'Organizational/Individual Capacity',\n",
    "    'Technical Know-how': 'Technical Know-how',\n",
    "}\n",
    "KNOWLEDGE_TYPES = [\n",
    "    'Policy Entrepreneurship (leadership)',\n",
    "    'Policy Implementation & Coordinating Mechanism',\n",
    "    'Organizational/Individual Capacity',\n",
    "    'Technical Know-how',\n",
    "]\n",
    "\n",
    "def parse_nlm_kts(raw):\n",
    "    \"\"\"Parse knowledge types from NLM string like 'TypeA(desc)TypeB(desc)'.\"\"\"\n",
    "    parts = re.findall(r'([^(]+?)\\(([^)]+)\\)', raw)\n",
    "    types = []\n",
    "    for name, _ in parts:\n",
    "        canonical = KT_MAP.get(name.strip())\n",
    "        if canonical and canonical not in types:\n",
    "            types.append(canonical)\n",
    "    return types if types else ['Unknown']\n",
    "\n",
    "# --- NLM policy counting ---\n",
    "def count_nlm_policies(raw):\n",
    "    \"\"\"Count individual policies from NLM free-text description.\"\"\"\n",
    "    if not raw or '(This chapter' in raw:\n",
    "        return 0\n",
    "    return len(re.findall(r'\\((?:\\d{4}|\\d{4}s)(?:-\\d{2,4}s?)?,\\s*[^)]+\\)', raw))\n",
    "\n",
    "# --- Pipeline policy counting ---\n",
    "def count_pipeline_policies(entry):\n",
    "    \"\"\"Count real policies from a pipeline chapter entry.\"\"\"\n",
    "    kp = entry.get('korean_policies', 'Not Applicable')\n",
    "    if isinstance(kp, list):\n",
    "        return len([p for p in kp if p.get('policy_name') != 'Not Applicable'])\n",
    "    return 0\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Parse NotebookLM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse NLM CSV into structured data\n",
    "nlm_data = []\n",
    "for row in nlm_raw:\n",
    "    project_info = row.get('Project Info (No. & Chapter)', '').strip()\n",
    "    if not project_info:\n",
    "        continue  # skip header / non-data rows\n",
    "\n",
    "    chapter_raw = row.get('chapter', '').strip()\n",
    "    # VNM titles embed page numbers like \"...___34\"\n",
    "    page_match = re.search(r'_+(\\d+)\\s*$', chapter_raw)\n",
    "    page_num = int(page_match.group(1)) if page_match else None\n",
    "    title_clean = re.sub(r'\\s*_+\\d+\\s*$', '', chapter_raw).strip()\n",
    "\n",
    "    sector_raw = row.get('Sector & Keywords (Taxonomy)', '').strip()\n",
    "    kt_raw = row.get('Type of Knowledge', '').strip()\n",
    "    pol_raw = row.get(\n",
    "        'Korean Policy / Program Entities (Title, Year, Institution, Contents)', '').strip()\n",
    "\n",
    "    nlm_data.append({\n",
    "        'report_short': row['project No'].strip()[:8],\n",
    "        'title': title_clean,\n",
    "        'page_num': page_num,\n",
    "        'sectors': parse_nlm_sectors(sector_raw),\n",
    "        'primary_sector': parse_nlm_sectors(sector_raw)[0],\n",
    "        'knowledge_types': parse_nlm_kts(kt_raw),\n",
    "        'primary_kt': parse_nlm_kts(kt_raw)[0],\n",
    "        'n_policies': count_nlm_policies(pol_raw),\n",
    "    })\n",
    "\n",
    "# Summary table\n",
    "print(f'NotebookLM: {len(nlm_data)} analyzed chapters')\n",
    "print()\n",
    "nlm_df = pd.DataFrame([{\n",
    "    'Report': d['report_short'],\n",
    "    'Chapter': d['title'][:50],\n",
    "    'Primary Sector': d['primary_sector'],\n",
    "    'Primary KT': d['primary_kt'].split('(')[0].strip()[:20],\n",
    "    '#KT': len(d['knowledge_types']),\n",
    "    '#Pol': d['n_policies'],\n",
    "} for d in nlm_data])\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 200)\n",
    "print(nlm_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Chapter Matching & Aggregation\n",
    "\n",
    "NLM analyzed top-level chapters while our pipeline analyzed sub-chapters.\n",
    "We match by:\n",
    "- **VNM**: page numbers embedded in NLM chapter titles\n",
    "- **QAT, SLV, KAZ**: detect section boundaries (where sub-chapter numbering restarts)\n",
    "  and map NLM chapters to sections in page order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Section boundary detection ---\n",
    "def detect_section_starts(chapters):\n",
    "    \"\"\"Detect section boundaries by finding where sub-chapter numbering restarts.\"\"\"\n",
    "    sorted_chs = sorted(chapters, key=lambda d: d['page_start'])\n",
    "    starts = []\n",
    "    prev_num = 99  # high initial so first numbered chapter triggers\n",
    "    for ch in sorted_chs:\n",
    "        title = ch['chapter_title'].strip()\n",
    "        m = re.match(r'^(\\d+|[A-Z])[\\.)\\s]', title)\n",
    "        if m:\n",
    "            tok = m.group(1)\n",
    "            num = int(tok) if tok.isdigit() else (ord(tok) - ord('A') + 1)\n",
    "            if num <= prev_num and prev_num > 1:\n",
    "                starts.append(ch['page_start'])\n",
    "            prev_num = num\n",
    "        elif re.match(r'^(summary|executive summary)', title, re.I):\n",
    "            starts.append(ch['page_start'])\n",
    "            prev_num = 0\n",
    "    return sorted(set(starts))\n",
    "\n",
    "# --- Match NLM chapters to pipeline sub-chapters ---\n",
    "nlm_by_report = defaultdict(list)\n",
    "for e in nlm_data:\n",
    "    nlm_by_report[e['report_short']].append(e)\n",
    "\n",
    "for rs, entries in nlm_by_report.items():\n",
    "    report_id = REPORT_MAP[rs]\n",
    "    chs = sorted(\n",
    "        [d for d in pipeline['Claude Sonnet 4']\n",
    "         if 'error' not in d and d['report_id'] == report_id],\n",
    "        key=lambda d: d['page_start'])\n",
    "\n",
    "    # VNM: use page numbers from NLM titles\n",
    "    if all(e['page_num'] is not None for e in entries):\n",
    "        for e in entries:\n",
    "            e['anchor_page'] = e['page_num']\n",
    "    else:\n",
    "        # Other reports: detect structural sections, map to NLM in order\n",
    "        sections = detect_section_starts(chs)\n",
    "        extra = len(sections) - len(entries)\n",
    "        if extra == 0:\n",
    "            mapped = sections\n",
    "        elif extra == 1:\n",
    "            mapped = sections[1:]  # skip intro/overview\n",
    "        elif extra == 2:\n",
    "            mapped = sections[1:-1]  # skip intro + appendix\n",
    "        else:\n",
    "            mapped = sections[:len(entries)]  # fallback\n",
    "        for i, e in enumerate(entries):\n",
    "            e['anchor_page'] = mapped[i] if i < len(mapped) else None\n",
    "\n",
    "# Define page ranges and assign sub-chapters from ALL models\n",
    "for rs, entries in nlm_by_report.items():\n",
    "    entries.sort(key=lambda x: x.get('anchor_page') or 0)\n",
    "    report_id = REPORT_MAP[rs]\n",
    "\n",
    "    for i, e in enumerate(entries):\n",
    "        p_start = e['anchor_page']\n",
    "        p_end = (entries[i+1]['anchor_page'] - 1\n",
    "                 if i + 1 < len(entries) and entries[i+1].get('anchor_page')\n",
    "                 else 9999)\n",
    "        e['page_range'] = (p_start, p_end)\n",
    "\n",
    "        # Match sub-chapters from each pipeline model\n",
    "        e['matched'] = {}\n",
    "        for model in MODEL_LABELS:\n",
    "            e['matched'][model] = [\n",
    "                d for d in pipeline[model]\n",
    "                if 'error' not in d\n",
    "                and d['report_id'] == report_id\n",
    "                and p_start <= d['page_start'] <= p_end\n",
    "            ]\n",
    "\n",
    "# Show mapping summary\n",
    "print('Chapter Matching Summary')\n",
    "print('=' * 85)\n",
    "for rs in sorted(nlm_by_report.keys()):\n",
    "    entries = nlm_by_report[rs]\n",
    "    report_id = REPORT_MAP[rs]\n",
    "    total_pipeline = len([d for d in pipeline['Claude Sonnet 4']\n",
    "                          if 'error' not in d and d['report_id'] == report_id])\n",
    "    total_matched = sum(len(e['matched']['Claude Sonnet 4']) for e in entries)\n",
    "    print(f'\\n{rs} ({total_matched}/{total_pipeline} sub-chapters matched):')\n",
    "    for e in entries:\n",
    "        p_s, p_e = e['page_range']\n",
    "        n = len(e['matched']['Claude Sonnet 4'])\n",
    "        print(f'  p{p_s:>3}-{p_e:>4} ({n:2d} sub-ch) | {e[\"title\"][:55]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate pipeline sub-chapters per NLM chapter ---\n",
    "def aggregate_subchapters(chapters, model):\n",
    "    \"\"\"Aggregate sub-chapters: majority-vote sector & KT, sum policies.\"\"\"\n",
    "    if not chapters:\n",
    "        return {'sector': 'N/A', 'kt': 'N/A', 'policies': 0, 'n': 0}\n",
    "\n",
    "    sectors = []\n",
    "    for d in chapters:\n",
    "        s_list = d['taxonomy_classification']['sectors']\n",
    "        if s_list:\n",
    "            sectors.append(normalize_sector(s_list[0]['sector']))\n",
    "    primary_sector = Counter(sectors).most_common(1)[0][0] if sectors else 'N/A'\n",
    "\n",
    "    kts = [d['taxonomy_classification']['knowledge_type'] for d in chapters]\n",
    "    primary_kt = Counter(kts).most_common(1)[0][0] if kts else 'N/A'\n",
    "\n",
    "    total_policies = sum(count_pipeline_policies(d) for d in chapters)\n",
    "\n",
    "    return {\n",
    "        'sector': primary_sector,\n",
    "        'kt': primary_kt,\n",
    "        'policies': total_policies,\n",
    "        'n': len(chapters),\n",
    "    }\n",
    "\n",
    "# Build comparison table: NLM vs aggregated pipeline (Claude Sonnet as primary)\n",
    "rows = []\n",
    "for rs in sorted(nlm_by_report.keys()):\n",
    "    for e in nlm_by_report[rs]:\n",
    "        row = {\n",
    "            'Report': rs,\n",
    "            'NLM Chapter': e['title'][:45],\n",
    "            'NLM Sector': e['primary_sector'].split(') ')[-1][:15],\n",
    "            'NLM KT': e['primary_kt'].split('(')[0].strip()[:15],\n",
    "            'NLM #Pol': e['n_policies'],\n",
    "        }\n",
    "        agg = aggregate_subchapters(e['matched']['Claude Sonnet 4'], 'Claude Sonnet 4')\n",
    "        row['Sonnet Sector'] = agg['sector'].split(') ')[-1][:15] if agg['sector'] != 'N/A' else 'N/A'\n",
    "        row['Sonnet KT'] = agg['kt'].split('(')[0].strip()[:15] if agg['kt'] != 'N/A' else 'N/A'\n",
    "        row['Sonnet #Pol'] = agg['policies']\n",
    "        row['#Sub'] = agg['n']\n",
    "        rows.append(row)\n",
    "\n",
    "comp_df = pd.DataFrame(rows)\n",
    "print('=== Chapter-Level Comparison: NLM vs Claude Sonnet (aggregated) ===')\n",
    "print()\n",
    "print(comp_df.to_string(index=False))\n",
    "\n",
    "# Agreement rates\n",
    "print()\n",
    "sector_match = sum(1 for _, r in comp_df.iterrows()\n",
    "                   if r['NLM Sector'] == r['Sonnet Sector'])\n",
    "kt_match = sum(1 for _, r in comp_df.iterrows()\n",
    "               if r['NLM KT'] == r['Sonnet KT'])\n",
    "n = len(comp_df)\n",
    "print(f'Sector agreement: {sector_match}/{n} ({sector_match/n*100:.1f}%)')\n",
    "print(f'Knowledge type agreement: {kt_match}/{n} ({kt_match/n*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Report-Level Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sector distribution by report ---\n",
    "# For each source and report, count primary sector assignments\n",
    "\n",
    "def get_report_sector_counts(source_label):\n",
    "    \"\"\"Return {report_short: Counter(sector -> count)} for a given source.\"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    if source_label == 'NotebookLM':\n",
    "        for d in nlm_data:\n",
    "            counts[d['report_short']][d['primary_sector']] += 1\n",
    "    else:\n",
    "        for d in pipeline[source_label]:\n",
    "            if 'error' not in d:\n",
    "                rs = d['report_id'][:8]\n",
    "                sectors = d['taxonomy_classification']['sectors']\n",
    "                if sectors:\n",
    "                    counts[rs][normalize_sector(sectors[0]['sector'])] += 1\n",
    "    return counts\n",
    "\n",
    "all_report_shorts = sorted(REPORT_MAP.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "for ax, rs in zip(axes.flat, all_report_shorts):\n",
    "    sector_counts = {}\n",
    "    for label in ALL_LABELS:\n",
    "        rc = get_report_sector_counts(label)\n",
    "        total = sum(rc[rs].values()) or 1\n",
    "        sector_counts[label] = {s: rc[rs].get(s, 0) / total * 100 for s in SECTOR_NAMES}\n",
    "\n",
    "    x = np.arange(len(SECTOR_NAMES))\n",
    "    width = 0.2\n",
    "    for i, label in enumerate(ALL_LABELS):\n",
    "        vals = [sector_counts[label].get(s, 0) for s in SECTOR_NAMES]\n",
    "        ax.bar(x + i * width, vals, width, label=label, color=COLORS[label])\n",
    "\n",
    "    ax.set_title(rs, fontweight='bold')\n",
    "    ax.set_ylabel('% of chapters')\n",
    "    ax.set_xticks(x + 1.5 * width)\n",
    "    short_labels = [f'({s.split(\")\")[0].split(\"(\")[1]})' for s in SECTOR_NAMES]\n",
    "    ax.set_xticklabels(short_labels, fontsize=8)\n",
    "    ax.set_ylim(0, 105)\n",
    "\n",
    "axes[0, 0].legend(fontsize=7, loc='upper right')\n",
    "fig.suptitle('Sector Distribution by Report (% of chapters)', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Knowledge type distribution (overall) ---\n",
    "def get_overall_kt_counts(source_label):\n",
    "    counts = Counter()\n",
    "    if source_label == 'NotebookLM':\n",
    "        for d in nlm_data:\n",
    "            # NLM can have multiple KTs per chapter — count each once\n",
    "            for kt in d['knowledge_types']:\n",
    "                counts[kt] += 1\n",
    "    else:\n",
    "        for d in pipeline[source_label]:\n",
    "            if 'error' not in d:\n",
    "                counts[d['taxonomy_classification']['knowledge_type']] += 1\n",
    "    return counts\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "x = np.arange(len(KNOWLEDGE_TYPES))\n",
    "width = 0.18\n",
    "\n",
    "for i, label in enumerate(ALL_LABELS):\n",
    "    kt_counts = get_overall_kt_counts(label)\n",
    "    total = sum(kt_counts.values()) or 1\n",
    "    pcts = [kt_counts.get(kt, 0) / total * 100 for kt in KNOWLEDGE_TYPES]\n",
    "    ax.bar(x + i * width, pcts, width, label=label, color=COLORS[label])\n",
    "\n",
    "ax.set_ylabel('% of classifications')\n",
    "ax.set_title('Knowledge Type Distribution (all chapters, normalized %)')\n",
    "ax.set_xticks(x + 1.5 * width)\n",
    "kt_short = ['Policy\\nEntrepreneurship', 'Policy\\nImplementation',\n",
    "             'Organizational\\nCapacity', 'Technical\\nKnow-how']\n",
    "ax.set_xticklabels(kt_short, fontsize=9)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print raw counts\n",
    "print('Raw knowledge type counts:')\n",
    "for label in ALL_LABELS:\n",
    "    kt_counts = get_overall_kt_counts(label)\n",
    "    total = sum(kt_counts.values())\n",
    "    parts = ', '.join(f'{kt.split(\"(\")[0].strip()[:15]}: {kt_counts.get(kt, 0)}'\n",
    "                      for kt in KNOWLEDGE_TYPES)\n",
    "    print(f'  {label}: {parts} (total={total})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Policy extraction density by report ---\n",
    "def get_report_policy_counts(source_label):\n",
    "    counts = Counter()\n",
    "    n_chapters = Counter()\n",
    "    if source_label == 'NotebookLM':\n",
    "        for d in nlm_data:\n",
    "            counts[d['report_short']] += d['n_policies']\n",
    "            n_chapters[d['report_short']] += 1\n",
    "    else:\n",
    "        for d in pipeline[source_label]:\n",
    "            if 'error' not in d:\n",
    "                rs = d['report_id'][:8]\n",
    "                counts[rs] += count_pipeline_policies(d)\n",
    "                n_chapters[rs] += 1\n",
    "    return counts, n_chapters\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total policies per report\n",
    "x = np.arange(len(all_report_shorts))\n",
    "width = 0.18\n",
    "for i, label in enumerate(ALL_LABELS):\n",
    "    pol_counts, _ = get_report_policy_counts(label)\n",
    "    vals = [pol_counts.get(rs, 0) for rs in all_report_shorts]\n",
    "    ax1.bar(x + i * width, vals, width, label=label, color=COLORS[label])\n",
    "ax1.set_ylabel('Total policies extracted')\n",
    "ax1.set_title('Total Policy Count by Report')\n",
    "ax1.set_xticks(x + 1.5 * width)\n",
    "ax1.set_xticklabels(all_report_shorts, fontsize=9)\n",
    "ax1.legend(fontsize=8)\n",
    "\n",
    "# Policies per chapter (density)\n",
    "for i, label in enumerate(ALL_LABELS):\n",
    "    pol_counts, n_ch = get_report_policy_counts(label)\n",
    "    vals = [pol_counts.get(rs, 0) / max(n_ch.get(rs, 1), 1) for rs in all_report_shorts]\n",
    "    ax2.bar(x + i * width, vals, width, label=label, color=COLORS[label])\n",
    "ax2.set_ylabel('Policies per chapter')\n",
    "ax2.set_title('Policy Extraction Density by Report')\n",
    "ax2.set_xticks(x + 1.5 * width)\n",
    "ax2.set_xticklabels(all_report_shorts, fontsize=9)\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement summary across all models vs NLM at chapter level\n",
    "print('=' * 70)\n",
    "print('NLM vs PIPELINE COMPARISON SUMMARY')\n",
    "print('=' * 70)\n",
    "print()\n",
    "\n",
    "# 1. Chapter-level agreement for each model\n",
    "print('1. CHAPTER-LEVEL AGREEMENT (NLM vs aggregated pipeline)')\n",
    "print('-' * 50)\n",
    "for model in MODEL_LABELS:\n",
    "    s_agree = 0\n",
    "    kt_agree = 0\n",
    "    n_compared = 0\n",
    "    for rs in nlm_by_report:\n",
    "        for e in nlm_by_report[rs]:\n",
    "            agg = aggregate_subchapters(e['matched'][model], model)\n",
    "            if agg['n'] == 0:\n",
    "                continue\n",
    "            n_compared += 1\n",
    "            if e['primary_sector'] == agg['sector']:\n",
    "                s_agree += 1\n",
    "            if e['primary_kt'] == agg['kt']:\n",
    "                kt_agree += 1\n",
    "    print(f'  {model}:')\n",
    "    print(f'    Sector:  {s_agree}/{n_compared} ({s_agree/n_compared*100:.1f}%)')\n",
    "    print(f'    KT:      {kt_agree}/{n_compared} ({kt_agree/n_compared*100:.1f}%)')\n",
    "print()\n",
    "\n",
    "# 2. Overall distribution comparison\n",
    "print('2. KNOWLEDGE TYPE DISTRIBUTION COMPARISON')\n",
    "print('-' * 50)\n",
    "for label in ALL_LABELS:\n",
    "    kt_counts = get_overall_kt_counts(label)\n",
    "    total = sum(kt_counts.values())\n",
    "    top = kt_counts.most_common(1)[0]\n",
    "    print(f'  {label}: top KT = \"{top[0].split(\"(\")[0].strip()}\"'\n",
    "          f' ({top[1]}/{total}, {top[1]/total*100:.0f}%)')\n",
    "print()\n",
    "\n",
    "# 3. Policy extraction\n",
    "print('3. POLICY EXTRACTION TOTALS')\n",
    "print('-' * 50)\n",
    "for label in ALL_LABELS:\n",
    "    pol_counts, n_ch = get_report_policy_counts(label)\n",
    "    total_pol = sum(pol_counts.values())\n",
    "    total_ch = sum(n_ch.values())\n",
    "    density = total_pol / total_ch if total_ch else 0\n",
    "    print(f'  {label}: {total_pol} policies from {total_ch} chapters'\n",
    "          f' ({density:.1f} per chapter)')\n",
    "print()\n",
    "\n",
    "# 4. Key observations\n",
    "print('4. KEY OBSERVATIONS')\n",
    "print('-' * 50)\n",
    "print('  - NLM analyzed 19 top-level chapters; pipeline analyzed 112 sub-chapters')\n",
    "print('  - NLM assigns multiple knowledge types per chapter;')\n",
    "print('    pipeline assigns exactly one')\n",
    "print('  - NLM policy density is higher (fewer chapters, focused extraction)')\n",
    "print('  - Sector agreement depends on how sub-chapter aggregation resolves')\n",
    "print('    majority votes — heterogeneous sub-chapters may produce different')\n",
    "print('    primary sectors than NLM\\'s holistic classification')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}